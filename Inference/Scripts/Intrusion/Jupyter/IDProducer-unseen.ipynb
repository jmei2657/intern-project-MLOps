{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e3d73-c77b-4be9-8e82-002aa470a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a19516-b45d-46fc-be14-0ac07d96b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.0.1\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.8.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.21.6)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e699f1e-faf8-47c8-9d5a-4312cea355a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96840e8-2340-4b3a-a55a-c89a44ec20ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.9/site-packages (2.0.31)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26edc69a-2cc2-4e2f-8d94-c010c7b19acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from scipy.special import boxcox\n",
    "import numpy as np\n",
    "import uuid\n",
    "import csv\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49980b4e-cc0d-4a24-ae0d-f6e01be0b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform normalization\n",
    "def zscore_normalization(df, name, preprocess_info):\n",
    "    pair = preprocess_info[name.lower()]\n",
    "    df[name] = (df[name] - pair[0]) / pair[1]\n",
    "        \n",
    "#Encode text \n",
    "def encode_text(df, name, preprocess_info):\n",
    "    import base64\n",
    "    import pickle\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    encoded_string = preprocess_info[name.lower()]\n",
    "    decoded_bytes = base64.b64decode(encoded_string)\n",
    "    enc = pickle.loads(decoded_bytes)\n",
    "    \n",
    "    data = enc.fit_transform(df[name].values.reshape(-1,1))\n",
    "    df[name] = data.flatten()\n",
    "        \n",
    "#Data preprocessing\n",
    "def preprocess(df):\n",
    "    with engine.connect() as conn: \n",
    "        query = text('SELECT * FROM metadata_table_intrusion ORDER BY version DESC LIMIT 1')\n",
    "        data = pd.read_sql_query(query, conn)\n",
    "        row = data.iloc[0]\n",
    "        factors = row['factor']\n",
    "    \n",
    "    for i in df.columns:\n",
    "        t = (df[i].dtype)\n",
    "        if i != 'outcome' and factors[i.lower()] == None:\n",
    "            df.drop(columns=i, inplace=True)\n",
    "        elif i != 'outcome':\n",
    "            if (t == int or t == float):\n",
    "                zscore_normalization(df, i, factors)\n",
    "            else:\n",
    "                df[i] = df[i].astype(str)\n",
    "                encode_text(df, i, factors)\n",
    "    return df\n",
    "\n",
    "def column_names():\n",
    "    \"\"\"Reads column names for dataframe into array\"\"\"\n",
    "    file_path = 'https://raw.githubusercontent.com/tsimhadri-ews/internproject/intrusion-detection-0/src/kddcup.names.txt'\n",
    "    req = requests.get(file_path)\n",
    "    arr = req.text.split('\\n')[1:-1]\n",
    "    cols = [a[0:a.index(\":\")] for a in arr]\n",
    "    cols.append(\"outcome\")\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a6cd2e-ace1-4531-8178-d387bbe0fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_secret():\n",
    "\n",
    "    secret_name = \"DBCreds\"\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "    # Parse the secret string to get the credentials\n",
    "    secret_dict = json.loads(secret)\n",
    "    username = secret_dict['username']\n",
    "    password = secret_dict['password']\n",
    "    host = secret_dict['host']\n",
    "    port = secret_dict['port']\n",
    "    dbname = secret_dict['dbname']\n",
    "\n",
    "    return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "(user,pswd,host,port,db) = get_secret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9a4be0-ec21-45bf-a7d2-75d81beeb38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connecting to PostgreSQL...\n",
      "INFO:root:PostgreSQL connection established successfully.\n",
      "INFO:root:Creating Kafka producers...\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=18.212.131.89:9092 <connecting> [IPv4 ('18.212.131.89', 9092)]>: connecting to 18.212.131.89:9092 [('18.212.131.89', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=18.212.131.89:9092 <connecting> [IPv4 ('18.212.131.89', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=18.212.131.89:9092 <connecting> [IPv4 ('18.212.131.89', 9092)]>: connecting to 18.212.131.89:9092 [('18.212.131.89', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=18.212.131.89:9092 <connecting> [IPv4 ('18.212.131.89', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:root:Sent row 1 to KServe Kafka topic\n",
      "INFO:root:Sent row 1 to PostgreSQL Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ec2-18-212-131-89.compute-1.amazonaws.com:9092 <connecting> [IPv4 ('18.212.131.89', 9092)]>: connecting to ec2-18-212-131-89.compute-1.amazonaws.com:9092 [('18.212.131.89', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ec2-18-212-131-89.compute-1.amazonaws.com:9092 <connecting> [IPv4 ('18.212.131.89', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=ec2-18-212-131-89.compute-1.amazonaws.com:9093 <connecting> [IPv4 ('18.212.131.89', 9093)]>: connecting to ec2-18-212-131-89.compute-1.amazonaws.com:9093 [('18.212.131.89', 9093) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=18.212.131.89:9092 <connected> [IPv4 ('18.212.131.89', 9092)]>: Closing connection. \n",
      "INFO:kafka.conn:<BrokerConnection node_id=1 host=ec2-18-212-131-89.compute-1.amazonaws.com:9093 <connecting> [IPv4 ('18.212.131.89', 9093)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=18.212.131.89:9092 <connected> [IPv4 ('18.212.131.89', 9092)]>: Closing connection. \n",
      "INFO:root:Sent row 2 to KServe Kafka topic\n",
      "INFO:root:Sent row 2 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 3 to KServe Kafka topic\n",
      "INFO:root:Sent row 3 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 4 to KServe Kafka topic\n",
      "INFO:root:Sent row 4 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5 to KServe Kafka topic\n",
      "INFO:root:Sent row 5 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 6 to KServe Kafka topic\n",
      "INFO:root:Sent row 6 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 7 to KServe Kafka topic\n",
      "INFO:root:Sent row 7 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 8 to KServe Kafka topic\n",
      "INFO:root:Sent row 8 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 9 to KServe Kafka topic\n",
      "INFO:root:Sent row 9 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 10 to KServe Kafka topic\n",
      "INFO:root:Sent row 10 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 11 to KServe Kafka topic\n",
      "INFO:root:Sent row 11 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 12 to KServe Kafka topic\n",
      "INFO:root:Sent row 12 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 13 to KServe Kafka topic\n",
      "INFO:root:Sent row 13 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 14 to KServe Kafka topic\n",
      "INFO:root:Sent row 14 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 15 to KServe Kafka topic\n",
      "INFO:root:Sent row 15 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 16 to KServe Kafka topic\n",
      "INFO:root:Sent row 16 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 17 to KServe Kafka topic\n",
      "INFO:root:Sent row 17 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 18 to KServe Kafka topic\n",
      "INFO:root:Sent row 18 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 19 to KServe Kafka topic\n",
      "INFO:root:Sent row 19 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 20 to KServe Kafka topic\n",
      "INFO:root:Sent row 20 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 21 to KServe Kafka topic\n",
      "INFO:root:Sent row 21 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 22 to KServe Kafka topic\n",
      "INFO:root:Sent row 22 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 23 to KServe Kafka topic\n",
      "INFO:root:Sent row 23 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 24 to KServe Kafka topic\n",
      "INFO:root:Sent row 24 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 25 to KServe Kafka topic\n",
      "INFO:root:Sent row 25 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 26 to KServe Kafka topic\n",
      "INFO:root:Sent row 26 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 27 to KServe Kafka topic\n",
      "INFO:root:Sent row 27 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 28 to KServe Kafka topic\n",
      "INFO:root:Sent row 28 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 29 to KServe Kafka topic\n",
      "INFO:root:Sent row 29 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 30 to KServe Kafka topic\n",
      "INFO:root:Sent row 30 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 31 to KServe Kafka topic\n",
      "INFO:root:Sent row 31 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 32 to KServe Kafka topic\n",
      "INFO:root:Sent row 32 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 33 to KServe Kafka topic\n",
      "INFO:root:Sent row 33 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 34 to KServe Kafka topic\n",
      "INFO:root:Sent row 34 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 35 to KServe Kafka topic\n",
      "INFO:root:Sent row 35 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 36 to KServe Kafka topic\n",
      "INFO:root:Sent row 36 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 37 to KServe Kafka topic\n",
      "INFO:root:Sent row 37 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 38 to KServe Kafka topic\n",
      "INFO:root:Sent row 38 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 39 to KServe Kafka topic\n",
      "INFO:root:Sent row 39 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 40 to KServe Kafka topic\n",
      "INFO:root:Sent row 40 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 41 to KServe Kafka topic\n",
      "INFO:root:Sent row 41 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 42 to KServe Kafka topic\n",
      "INFO:root:Sent row 42 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 43 to KServe Kafka topic\n",
      "INFO:root:Sent row 43 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 44 to KServe Kafka topic\n",
      "INFO:root:Sent row 44 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 45 to KServe Kafka topic\n",
      "INFO:root:Sent row 45 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 46 to KServe Kafka topic\n",
      "INFO:root:Sent row 46 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 47 to KServe Kafka topic\n",
      "INFO:root:Sent row 47 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 48 to KServe Kafka topic\n",
      "INFO:root:Sent row 48 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 49 to KServe Kafka topic\n",
      "INFO:root:Sent row 49 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 50 to KServe Kafka topic\n",
      "INFO:root:Sent row 50 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 51 to KServe Kafka topic\n",
      "INFO:root:Sent row 51 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 52 to KServe Kafka topic\n",
      "INFO:root:Sent row 52 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 53 to KServe Kafka topic\n",
      "INFO:root:Sent row 53 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 54 to KServe Kafka topic\n",
      "INFO:root:Sent row 54 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 55 to KServe Kafka topic\n",
      "INFO:root:Sent row 55 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 56 to KServe Kafka topic\n",
      "INFO:root:Sent row 56 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 57 to KServe Kafka topic\n",
      "INFO:root:Sent row 57 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 58 to KServe Kafka topic\n",
      "INFO:root:Sent row 58 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 59 to KServe Kafka topic\n",
      "INFO:root:Sent row 59 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 60 to KServe Kafka topic\n",
      "INFO:root:Sent row 60 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 61 to KServe Kafka topic\n",
      "INFO:root:Sent row 61 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 62 to KServe Kafka topic\n",
      "INFO:root:Sent row 62 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 63 to KServe Kafka topic\n",
      "INFO:root:Sent row 63 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 64 to KServe Kafka topic\n",
      "INFO:root:Sent row 64 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 65 to KServe Kafka topic\n",
      "INFO:root:Sent row 65 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 66 to KServe Kafka topic\n",
      "INFO:root:Sent row 66 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 67 to KServe Kafka topic\n",
      "INFO:root:Sent row 67 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 68 to KServe Kafka topic\n",
      "INFO:root:Sent row 68 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 69 to KServe Kafka topic\n",
      "INFO:root:Sent row 69 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 70 to KServe Kafka topic\n",
      "INFO:root:Sent row 70 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 71 to KServe Kafka topic\n",
      "INFO:root:Sent row 71 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 72 to KServe Kafka topic\n",
      "INFO:root:Sent row 72 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 73 to KServe Kafka topic\n",
      "INFO:root:Sent row 73 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 74 to KServe Kafka topic\n",
      "INFO:root:Sent row 74 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 75 to KServe Kafka topic\n",
      "INFO:root:Sent row 75 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 76 to KServe Kafka topic\n",
      "INFO:root:Sent row 76 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 77 to KServe Kafka topic\n",
      "INFO:root:Sent row 77 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 78 to KServe Kafka topic\n",
      "INFO:root:Sent row 78 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 79 to KServe Kafka topic\n",
      "INFO:root:Sent row 79 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 80 to KServe Kafka topic\n",
      "INFO:root:Sent row 80 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 81 to KServe Kafka topic\n",
      "INFO:root:Sent row 81 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 82 to KServe Kafka topic\n",
      "INFO:root:Sent row 82 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 83 to KServe Kafka topic\n",
      "INFO:root:Sent row 83 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 84 to KServe Kafka topic\n",
      "INFO:root:Sent row 84 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 85 to KServe Kafka topic\n",
      "INFO:root:Sent row 85 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 86 to KServe Kafka topic\n",
      "INFO:root:Sent row 86 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 87 to KServe Kafka topic\n",
      "INFO:root:Sent row 87 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 88 to KServe Kafka topic\n",
      "INFO:root:Sent row 88 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 89 to KServe Kafka topic\n",
      "INFO:root:Sent row 89 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 90 to KServe Kafka topic\n",
      "INFO:root:Sent row 90 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 91 to KServe Kafka topic\n",
      "INFO:root:Sent row 91 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 92 to KServe Kafka topic\n",
      "INFO:root:Sent row 92 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 93 to KServe Kafka topic\n",
      "INFO:root:Sent row 93 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 94 to KServe Kafka topic\n",
      "INFO:root:Sent row 94 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 95 to KServe Kafka topic\n",
      "INFO:root:Sent row 95 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 96 to KServe Kafka topic\n",
      "INFO:root:Sent row 96 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 97 to KServe Kafka topic\n",
      "INFO:root:Sent row 97 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 98 to KServe Kafka topic\n",
      "INFO:root:Sent row 98 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 99 to KServe Kafka topic\n",
      "INFO:root:Sent row 99 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 100 to KServe Kafka topic\n",
      "INFO:root:Sent row 100 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 101 to KServe Kafka topic\n",
      "INFO:root:Sent row 101 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 102 to KServe Kafka topic\n",
      "INFO:root:Sent row 102 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 103 to KServe Kafka topic\n",
      "INFO:root:Sent row 103 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 104 to KServe Kafka topic\n",
      "INFO:root:Sent row 104 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 105 to KServe Kafka topic\n",
      "INFO:root:Sent row 105 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 106 to KServe Kafka topic\n",
      "INFO:root:Sent row 106 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 107 to KServe Kafka topic\n",
      "INFO:root:Sent row 107 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 108 to KServe Kafka topic\n",
      "INFO:root:Sent row 108 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 109 to KServe Kafka topic\n",
      "INFO:root:Sent row 109 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 110 to KServe Kafka topic\n",
      "INFO:root:Sent row 110 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 111 to KServe Kafka topic\n",
      "INFO:root:Sent row 111 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 112 to KServe Kafka topic\n",
      "INFO:root:Sent row 112 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 113 to KServe Kafka topic\n",
      "INFO:root:Sent row 113 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 114 to KServe Kafka topic\n",
      "INFO:root:Sent row 114 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 115 to KServe Kafka topic\n",
      "INFO:root:Sent row 115 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 116 to KServe Kafka topic\n",
      "INFO:root:Sent row 116 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 117 to KServe Kafka topic\n",
      "INFO:root:Sent row 117 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 118 to KServe Kafka topic\n",
      "INFO:root:Sent row 118 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 119 to KServe Kafka topic\n",
      "INFO:root:Sent row 119 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 120 to KServe Kafka topic\n",
      "INFO:root:Sent row 120 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 121 to KServe Kafka topic\n",
      "INFO:root:Sent row 121 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 122 to KServe Kafka topic\n",
      "INFO:root:Sent row 122 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 123 to KServe Kafka topic\n",
      "INFO:root:Sent row 123 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 124 to KServe Kafka topic\n",
      "INFO:root:Sent row 124 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 125 to KServe Kafka topic\n",
      "INFO:root:Sent row 125 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 126 to KServe Kafka topic\n",
      "INFO:root:Sent row 126 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 127 to KServe Kafka topic\n",
      "INFO:root:Sent row 127 to PostgreSQL Kafka topic\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize logger\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Broker address\n",
    "brokers = [f'{host}:9092']\n",
    "\n",
    "# GitHub raw URL for the CSV file\n",
    "github_url = \"https://github.com/tsimhadri-ews/internproject/raw/main/Data/kddcup.data.corrected.zip\"\n",
    "\n",
    "# PostgreSQL database connection details\n",
    "db_config = {\n",
    "    'dbname': db,\n",
    "    'user': user,\n",
    "    'password': pswd,\n",
    "    'host': host,\n",
    "    'port': port\n",
    "}\n",
    "\n",
    "# PostgreSQL connection setup\n",
    "logger.info(\"Connecting to PostgreSQL...\")\n",
    "try:\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    logger.info(\"PostgreSQL connection established successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Kafka topics\n",
    "postgres_topic = \"id-postgresql\"\n",
    "kserve_topic = \"id-kserve\"\n",
    "\n",
    "# Download the CSV file from GitHub\n",
    "# logger.info(\"Downloading CSV file from GitHub...\")\n",
    "# #response = requests.get(github_url)\n",
    "# df = pd.DataFrame()\n",
    "# # Check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     logger.info(\"CSV file downloaded successfully.\")\n",
    "#     # Read the data into a pandas DataFrame\n",
    "\n",
    "#     cols = column_names()\n",
    "#     df = pd.read_csv(github_url, index_col=False, names=cols)\n",
    "    \n",
    "#     df.loc[df['outcome'] != \"normal.\", 'outcome']  =1\n",
    "#     df.loc[df['outcome'] == \"normal.\", 'outcome']  =0\n",
    "    \n",
    "#     unprocess_df = df\n",
    "#     raw_df = df\n",
    "#     df = preprocess(df)\n",
    "#     raw_df = raw_df[df.columns]\n",
    "#     raw_df = raw_df.drop(columns=['outcome'])\n",
    "#     df = df.drop(columns=['outcome'])\n",
    "#     logger.info(\"CSV file processed successfully.\")\n",
    "# else:\n",
    "#     logger.error(f\"Failed to download file: {response.status_code}\")\n",
    "#     exit()\n",
    "\n",
    "# df['uid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "# unprocess_df['uid'] = df['uid']\n",
    "# Create Kafka producers\n",
    "logger.info(\"Creating Kafka producers...\")\n",
    "postgres_producer = KafkaProducer(\n",
    "    bootstrap_servers=brokers,\n",
    "    value_serializer=lambda message: json.dumps({k: v for k, v in message.items()}).encode('utf-8'),\n",
    ")\n",
    "\n",
    "kserve_producer = KafkaProducer(\n",
    "    bootstrap_servers=brokers,\n",
    "    value_serializer=lambda message: json.dumps(message).encode('utf-8'),\n",
    ")\n",
    "try:\n",
    "    with engine.connect() as conn: \n",
    "        query = text(\"SELECT * FROM metadata_table_intrusion ORDER BY date DESC LIMIT 1;\")\n",
    "        order = pd.read_sql_query(query, conn)\n",
    "        order = order['factor'][0]\n",
    "        order.pop('version')\n",
    "        order = {key: value for key, value in order.items() if value is not None}\n",
    "        new_order = list(order.keys())\n",
    "        #print(new_order)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "new_order.append('uid')\n",
    "\n",
    "cols = column_names()\n",
    "# Send data to KServe and PostgreSQL\n",
    "df = pd.DataFrame()\n",
    "unprocess_df = pd.DataFrame()\n",
    "\n",
    "response = requests.get(github_url)\n",
    "zipfile = ZipFile(BytesIO(response.content))\n",
    "\n",
    "csv_filename = zipfile.namelist()[0]\n",
    "\n",
    "chunks = pd.read_csv(zipfile.open(csv_filename), names=cols, chunksize=1)\n",
    "count = 0\n",
    "#res = []\n",
    "for chunk in chunks:\n",
    "    chunk.loc[count, 'outcome'] = 1 if chunk.loc[count,'outcome'] != \"normal.\" else 0\n",
    "    #res.append(chunk.loc[count, 'outcome'])\n",
    "    unprocess_df = pd.concat([unprocess_df, chunk], ignore_index=True)\n",
    "    \n",
    "    data = preprocess(chunk)\n",
    "    data['uid'] = [str(uuid.uuid4())]\n",
    "    unprocess_df.loc[count,'uid'] = data.loc[count,'uid']\n",
    "    data = data.drop(columns=['outcome'])\n",
    "    data = data[new_order]\n",
    "    df = pd.concat([df, data], ignore_index=True)\n",
    "    row = data.iloc[0].to_dict()\n",
    "    kserve_producer.send(kserve_topic, row)\n",
    "    logger.info(f\"Sent row {count + 1} to KServe Kafka topic\")\n",
    "    \n",
    "    postgres_producer.send(postgres_topic, unprocess_df.iloc[count].to_dict())\n",
    "    logger.info(f\"Sent row {count + 1} to PostgreSQL Kafka topic\")\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "# print(unprocess_df)\n",
    "# for index, row in first_5_rows.iterrows():\n",
    "#     data = row.to_dict()\n",
    "#     # Send to KServe topic\n",
    "#     kserve_producer.send(kserve_topic, data)\n",
    "#     logger.info(f\"Sent row {index + 1} to KServe Kafka topic\")\n",
    "kserve_producer.flush()\n",
    "kserve_producer.close()\n",
    "# Flush data and close the producers\n",
    "\n",
    "# first_5_unprocess = unprocess_df.head(10)\n",
    "# for index, row in first_5_unprocess.iterrows():\n",
    "#     data = row.to_dict()\n",
    "#     # Send to PostgreSQL topic\n",
    "#     postgres_producer.send(postgres_topic, data)\n",
    "#     logger.info(f\"Sent row {index + 1} to PostgreSQL Kafka topic\")\n",
    "# # Flush data and close the producers\n",
    "postgres_producer.flush()\n",
    "postgres_producer.close()\n",
    "\n",
    "logger.info(f\"First {count} rows sent to PostgreSQL Kafka topic: {postgres_topic} and KServe Kafka topic: {kserve_topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356d914-8889-4d59-b956-ab7eb1016004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf8aea-1b1e-4e77-85af-e4770fa348aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
