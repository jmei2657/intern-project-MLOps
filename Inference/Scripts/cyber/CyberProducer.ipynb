{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd395bff-90b0-4f1c-a8b3-c576e8381bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.0.1\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.21.6)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be05beb3-99af-4d3e-955f-9acfa367bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_secret():\n",
    "\n",
    "    secret_name = \"DBCreds\"\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "    # Parse the secret string to get the credentials\n",
    "    secret_dict = json.loads(secret)\n",
    "    username = secret_dict['username']\n",
    "    password = secret_dict['password']\n",
    "    host = secret_dict['host']\n",
    "    port = secret_dict['port']\n",
    "    dbname = secret_dict['dbname']\n",
    "\n",
    "    return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "(user,pswd,host,port,db) = get_secret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae99062-a375-4c2d-89d6-66493c33d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from scipy.special import boxcox\n",
    "import numpy as np\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba6e7d4-54c3-45d0-a5cf-011a5367cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform normalization\n",
    "def zscore_normalization(df, name, preprocess_info):\n",
    "    pair = preprocess_info[name.lower()]\n",
    "    df[name] = (df[name] - pair[0]) / pair[1]\n",
    "        \n",
    "#Encode text \n",
    "def encode_text(df, name, preprocess_info):\n",
    "    import base64\n",
    "    import pickle\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    encoded_string = preprocess_info[name.lower()]\n",
    "    decoded_bytes = base64.b64decode(encoded_string)\n",
    "    enc = pickle.loads(decoded_bytes)\n",
    "    \n",
    "    data = enc.fit_transform(df[name].values.reshape(-1,1))\n",
    "    df[name] = data.flatten()\n",
    "        \n",
    "#Data preprocessing\n",
    "def preprocess(df):\n",
    "    with engine.connect() as conn: \n",
    "        query = text('SELECT * FROM metadata_table_cyber ORDER BY version DESC LIMIT 1')\n",
    "        data = pd.read_sql_query(query, conn)\n",
    "        row = data.iloc[0]\n",
    "        factors = row['factor']\n",
    "    \n",
    "    for i in df.columns:\n",
    "        t = (df[i].dtype)\n",
    "        if i != 'outcome' and factors[i.lower()] == None:\n",
    "            df.drop(columns=i, inplace=True)\n",
    "        elif i != 'outcome':\n",
    "            if t == 'int64' or t == 'float64':\n",
    "                df[i] = boxcox(df[i], 0.5)\n",
    "                zscore_normalization(df, i, factors)\n",
    "               \n",
    "            else:\n",
    "                df[i] = df[i].astype(str)\n",
    "                encode_text(df, i, factors)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6fc4aa-4dff-4655-a381-d84030d60802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connecting to PostgreSQL...\n",
      "INFO:root:PostgreSQL connection established successfully.\n",
      "INFO:root:Downloading CSV file from GitHub...\n",
      "INFO:root:CSV file downloaded successfully.\n",
      "INFO:root:CSV file processed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000    5.0\n",
      "5001    6.0\n",
      "5002    5.0\n",
      "5003    3.0\n",
      "5004    3.0\n",
      "5005    5.0\n",
      "5006    2.0\n",
      "5007    6.0\n",
      "5008    6.0\n",
      "5009    5.0\n",
      "5010    6.0\n",
      "Name: outcome, dtype: float64\n",
      "5000    5.0\n",
      "5001    6.0\n",
      "5002    5.0\n",
      "5003    3.0\n",
      "5004    3.0\n",
      "5005    5.0\n",
      "5006    2.0\n",
      "5007    6.0\n",
      "5008    6.0\n",
      "5009    5.0\n",
      "5010    6.0\n",
      "Name: outcome, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Kafka producers...\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=3.81.56.214:9092 <connecting> [IPv4 ('3.81.56.214', 9092)]>: connecting to 3.81.56.214:9092 [('3.81.56.214', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=3.81.56.214:9092 <connecting> [IPv4 ('3.81.56.214', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=3.81.56.214:9092 <connecting> [IPv4 ('3.81.56.214', 9092)]>: connecting to 3.81.56.214:9092 [('3.81.56.214', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=3.81.56.214:9092 <connecting> [IPv4 ('3.81.56.214', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:root:Sent row 5001 to KServe Kafka topic\n",
      "INFO:root:Sent row 5002 to KServe Kafka topic\n",
      "INFO:root:Sent row 5003 to KServe Kafka topic\n",
      "INFO:root:Sent row 5004 to KServe Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=2 host=ec2-3-81-56-214.compute-1.amazonaws.com:9094 <connecting> [IPv4 ('3.81.56.214', 9094)]>: connecting to ec2-3-81-56-214.compute-1.amazonaws.com:9094 [('3.81.56.214', 9094) IPv4]\n",
      "INFO:root:Sent row 5005 to KServe Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=2 host=ec2-3-81-56-214.compute-1.amazonaws.com:9094 <connecting> [IPv4 ('3.81.56.214', 9094)]>: Connection complete.\n",
      "INFO:root:Sent row 5006 to KServe Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=3.81.56.214:9092 <connected> [IPv4 ('3.81.56.214', 9092)]>: Closing connection. \n",
      "INFO:root:Sent row 5007 to KServe Kafka topic\n",
      "INFO:root:Sent row 5008 to KServe Kafka topic\n",
      "INFO:root:Sent row 5009 to KServe Kafka topic\n",
      "INFO:root:Sent row 5010 to KServe Kafka topic\n",
      "INFO:root:Sent row 5011 to KServe Kafka topic\n",
      "INFO:kafka.producer.kafka:Closing the Kafka producer with 9223372036.0 secs timeout.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=2 host=ec2-3-81-56-214.compute-1.amazonaws.com:9094 <connected> [IPv4 ('3.81.56.214', 9094)]>: Closing connection. \n",
      "INFO:root:Sent row 5001 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5002 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5003 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5004 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5005 to PostgreSQL Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ec2-3-81-56-214.compute-1.amazonaws.com:9092 <connecting> [IPv4 ('3.81.56.214', 9092)]>: connecting to ec2-3-81-56-214.compute-1.amazonaws.com:9092 [('3.81.56.214', 9092) IPv4]\n",
      "INFO:root:Sent row 5006 to PostgreSQL Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ec2-3-81-56-214.compute-1.amazonaws.com:9092 <connecting> [IPv4 ('3.81.56.214', 9092)]>: Connection complete.\n",
      "INFO:root:Sent row 5007 to PostgreSQL Kafka topic\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=3.81.56.214:9092 <connected> [IPv4 ('3.81.56.214', 9092)]>: Closing connection. \n",
      "INFO:root:Sent row 5008 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5009 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5010 to PostgreSQL Kafka topic\n",
      "INFO:root:Sent row 5011 to PostgreSQL Kafka topic\n",
      "INFO:kafka.producer.kafka:Closing the Kafka producer with 9223372036.0 secs timeout.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ec2-3-81-56-214.compute-1.amazonaws.com:9092 <connected> [IPv4 ('3.81.56.214', 9092)]>: Closing connection. \n",
      "INFO:root:First 5 rows sent to PostgreSQL Kafka topic: cyd-postgresql and KServe Kafka topic: cyd-kserve\n"
     ]
    }
   ],
   "source": [
    "# Initialize logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Broker address\n",
    "brokers = [f'{host}:9092']\n",
    "\n",
    "# GitHub raw URL for the CSV file\n",
    "\n",
    "github_url = 'https://github.com/tsimhadri-ews/internproject/raw/main/Data/UNSW_NB15_testing-set.csv.zip'\n",
    "\n",
    "\n",
    "# PostgreSQL database connection details\n",
    "db_config = {\n",
    "    'dbname': db,\n",
    "    'user': user,\n",
    "    'password': pswd,\n",
    "    'host': host,\n",
    "    'port': port\n",
    "}\n",
    "\n",
    "# PostgreSQL connection setup\n",
    "logger.info(\"Connecting to PostgreSQL...\")\n",
    "try:\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    logger.info(\"PostgreSQL connection established successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Kafka topics\n",
    "postgres_topic = \"cyd-postgresql\"\n",
    "kserve_topic = \"cyd-kserve\"\n",
    "\n",
    "# Download the CSV file from GitHub\n",
    "logger.info(\"Downloading CSV file from GitHub...\")\n",
    "response = requests.get(github_url)\n",
    "zipfile = ZipFile(BytesIO(response.content))\n",
    "\n",
    "csv_filename = zipfile.namelist()[0]\n",
    "\n",
    "response = requests.get(github_url)\n",
    "df = pd.DataFrame()\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    logger.info(\"CSV file downloaded successfully.\")\n",
    "    # Read the data into a pandas DataFrame\n",
    "\n",
    "    df = pd.read_csv(zipfile.open(csv_filename), index_col=False)\n",
    "    encoding_dict = {\n",
    "        'Analysis': 0.0,\n",
    "        'Backdoor': 1.0,\n",
    "        'DoS': 2.0,\n",
    "        'Exploits': 3.0,\n",
    "        'Fuzzers': 4.0,\n",
    "        'Generic': 5.0,\n",
    "        'Normal': 6.0,\n",
    "        'Reconnaissance': 7.0,\n",
    "        'Shellcode': 8.0,\n",
    "        'Worms': 9.0\n",
    "    }\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    df['outcome'] = df['attack_cat'].map(encoding_dict)\n",
    "    df = df.drop(columns=['attack_cat'])\n",
    "    unprocess_df = df.copy()\n",
    "    df = preprocess(df)\n",
    "    \n",
    "    #print(df['outcome'].loc[5000:5010])\n",
    "    #print(unprocess_df['outcome'].loc[5000:5010])\n",
    "    \n",
    "    df = df.drop(columns=['outcome'])\n",
    "    logger.info(\"CSV file processed successfully.\")\n",
    "else:\n",
    "    logger.error(f\"Failed to download file: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "df['uid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "unprocess_df['uid'] = df['uid']\n",
    "# Create Kafka producers\n",
    "logger.info(\"Creating Kafka producers...\")\n",
    "postgres_producer = KafkaProducer(\n",
    "    bootstrap_servers=brokers,\n",
    "    value_serializer=lambda message: json.dumps({k: v for k, v in message.items()}).encode('utf-8'),\n",
    ")\n",
    "\n",
    "kserve_producer = KafkaProducer(\n",
    "    bootstrap_servers=brokers,\n",
    "    value_serializer=lambda message: json.dumps(message).encode('utf-8'),\n",
    ")\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "#print(df.columns)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn: \n",
    "        query = text(\"SELECT * FROM metadata_table_cyber ORDER BY date DESC LIMIT 1;\")\n",
    "        order = pd.read_sql_query(query, conn)\n",
    "        order = order['factor'][0]\n",
    "        order.pop('version')\n",
    "        order = {key: value for key, value in order.items() if value is not None}\n",
    "        new_order = list(order.keys())\n",
    "        #print(new_order)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch data: {e}\")\n",
    "new_order.append('uid')\n",
    "df = df[new_order]\n",
    "\n",
    "# Send data to KServe and PostgreSQL\n",
    "first_5_rows = df.loc[0:5]\n",
    "for index, row in first_5_rows.iterrows():\n",
    "    data = row.to_dict()\n",
    "    # Send to KServe topic\n",
    "    kserve_producer.send(kserve_topic, data)\n",
    "    logger.info(f\"Sent row {index + 1} to KServe Kafka topic\")\n",
    "\n",
    "kserve_producer.flush()\n",
    "kserve_producer.close()\n",
    "# Flush data and close the producers\n",
    "unprocess_df.columns = unprocess_df.columns.str.lower()\n",
    "first_5_unprocess = unprocess_df.loc[0:5]\n",
    "for index, row in first_5_unprocess.iterrows():\n",
    "    data = row.to_dict()\n",
    "    # Send to PostgreSQL topic\n",
    "    postgres_producer.send(postgres_topic, data)\n",
    "    logger.info(f\"Sent row {index + 1} to PostgreSQL Kafka topic\")\n",
    "# Flush data and close the producers\n",
    "postgres_producer.flush()\n",
    "postgres_producer.close()\n",
    "\n",
    "logger.info(f\"First 5 rows sent to PostgreSQL Kafka topic: {postgres_topic} and KServe Kafka topic: {kserve_topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3c49a-ced1-4fc5-9cae-126b2509bbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
