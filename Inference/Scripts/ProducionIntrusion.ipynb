{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e58dfc-bfa3-47a5-97da-824242982194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.0.1\n",
      "  Downloading scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.8.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f07bf9-607f-453a-ae5d-cd1cbfb26d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fc5c1d-a368-4acd-9137-8558f6879ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy==1.4.45\n",
      "  Using cached SQLAlchemy-1.4.45-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/site-packages (from sqlalchemy==1.4.45) (3.0.3)\n",
      "Installing collected packages: sqlalchemy\n",
      "Successfully installed sqlalchemy-1.4.45\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy==1.4.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0691116e-1f04-486f-9fa6-94df2b7222a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition() -> bool:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import boto3\n",
    "    import json\n",
    "\n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "\n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}', connect_args={'connect_timeout': 60})\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT count(*) FROM intrusion_outcomes;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT count(*) FROM metadata_table_intrusion;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            meta_count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "\n",
    "    if (count % 5 == 0 and count != 0) or meta_count == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d1b730-676e-4ba9-8099-a816ea41f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    import base64\n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    # Dictionary to save mean, sd, and, encoder\n",
    "    preprocess_df = {'version':1}\n",
    "    \n",
    "    #Perform normalization\n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "        \n",
    "    #Encode text \n",
    "    def encode_text(df, name):\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "        enc = OrdinalEncoder()\n",
    "        data = enc.fit_transform(df[name].values.reshape(-1,1))\n",
    "        df[name] = data.flatten()\n",
    "        preprocess_df[name] = base64.b64encode(pickle.dumps(enc)).decode('utf-8')\n",
    "        # pickle.loads(a.encode('latin1'))\n",
    "        \n",
    "    #Data preprocessing\n",
    "    def preprocess(df):\n",
    "\n",
    "        for col in df.columns:\n",
    "            t = (df[col].dtype)\n",
    "            if col != 'outcome':\n",
    "                if (t == int or t == float):\n",
    "                    zscore_normalization(df, col)\n",
    "               \n",
    "                else:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    encode_text(df, col)\n",
    "                            \n",
    "        for col in df.columns:\n",
    "            if len(df[col].unique()) == 1:\n",
    "                df.drop(col, inplace=True,axis=1)\n",
    "                preprocess_df[col] = None\n",
    "\n",
    "        #df.loc[df['outcome'] != \"normal.\", 'outcome']  = 1\n",
    "        #df.loc[df['outcome'] == \"normal.\", 'outcome']  = 0\n",
    "        #encode_text(df, \"outcome\")\n",
    "        print(df.columns)\n",
    "        correlation = df.corrwith(df[\"outcome\"])\n",
    "      \n",
    "        \n",
    "        row = 0 \n",
    "        for num in correlation:\n",
    "            if num >= -0.05 and num <= 0.05:\n",
    "                preprocess_df[df.columns[row]] = None\n",
    "                df.drop(df.columns[row], axis=1, inplace=True)\n",
    "                row -= 1\n",
    "            row += 1\n",
    "        return df\n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}', connect_args={'connect_timeout': 60})\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM intrusion_data WHERE outcome != 2;')\n",
    "            chunksize = 10000  # Adjust chunksize as per your memory and performance needs\n",
    "            chunks = pd.read_sql_query(query, conn, chunksize=chunksize)\n",
    "            i = 1\n",
    "            for chunk in chunks:\n",
    "                # print(f\"chunk{i}\")\n",
    "                i = i + 1\n",
    "                features_df = pd.json_normalize(chunk['features'])\n",
    "                features_df['outcome'] = chunk['outcome']\n",
    "                df = pd.concat([df, features_df], ignore_index=True)\n",
    "\n",
    "                # Get current process (notebook process) ID\n",
    "                pid = os.getpid()\n",
    "                process = psutil.Process(pid)\n",
    "\n",
    "                # Get CPU and memory usage\n",
    "#                 cpu_percent = process.cpu_percent()\n",
    "#                 mem_info = process.memory_info()\n",
    "#                 mem_usage = mem_info.rss / (1024 * 1024)  # in MiB\n",
    "\n",
    "#                 print(f\"CPU Usage: {cpu_percent}%\")\n",
    "#                 print(f\"Memory Usage: {mem_usage} MiB\")\n",
    "                \n",
    "                \n",
    "                \n",
    "    except Exception as e:\n",
    "            print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "    print(df.columns)\n",
    "    \n",
    "    #df = df.drop(columns=['timestamp','uid'])\n",
    "    df = preprocess(df)\n",
    "    X = df.drop(columns=['outcome'])\n",
    "    y = df['outcome']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    bucket_name=\"intrusionpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    folder_path = './tmp/intrusion'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM metadata_table_intrusion ORDER BY version DESC LIMIT 1;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            version = data['version'].iloc[0] + 1\n",
    "    except Exception as e:\n",
    "        version = 1\n",
    "        \n",
    "    df.to_csv(\"./tmp/intrusion/intrusion_data.csv\")\n",
    "    s3_client.upload_file(\"./tmp/intrusion/intrusion_data.csv\", bucket_name, f\"version{version}/intrusion_dataset.csv\")\n",
    "    np.save(\"./tmp/intrusion/X_train.npy\",X_train)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/X_train.npy\", bucket_name, f\"version{version}/X_train.npy\")\n",
    "    np.save(\"./tmp/intrusion/y_train.npy\",y_train)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/y_train.npy\", bucket_name, f\"version{version}/y_train.npy\")\n",
    "    np.save(\"./tmp/intrusion/X_test.npy\",X_test)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/X_test.npy\", bucket_name, f\"version{version}/X_test.npy\")\n",
    "    np.save(\"./tmp/intrusion/y_test.npy\",y_test)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/y_test.npy\", bucket_name, f\"version{version}/y_test.npy\")\n",
    "        \n",
    "        \n",
    "    preprocess_df['version'] = version\n",
    "    mean_df = pd.DataFrame([preprocess_df])\n",
    "    meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), len(X.columns), json.dumps(df.dtypes.astype(str).to_dict()),mean_df.iloc[0].to_json()]], columns = ['version', 'date', 'features', 'types','factor'])\n",
    "    meta_df.to_sql(\"metadata_table_intrusion\", engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b33b2c1-7907-477f-a800-3026de9fa4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hot', 'flag', 'land', 'count', 'urgent', 'service', 'duration',\n",
      "       'num_root', 'dst_bytes', 'logged_in', 'src_bytes', 'srv_count',\n",
      "       'num_shells', 'root_shell', 'rerror_rate', 'serror_rate',\n",
      "       'su_attempted', 'diff_srv_rate', 'is_host_login', 'protocol_type',\n",
      "       'same_srv_rate', 'dst_host_count', 'is_guest_login', 'wrong_fragment',\n",
      "       'num_compromised', 'srv_rerror_rate', 'srv_serror_rate',\n",
      "       'num_access_files', 'num_failed_logins', 'num_outbound_cmds',\n",
      "       'dst_host_srv_count', 'num_file_creations', 'srv_diff_host_rate',\n",
      "       'dst_host_rerror_rate', 'dst_host_serror_rate',\n",
      "       'dst_host_diff_srv_rate', 'dst_host_same_srv_rate',\n",
      "       'dst_host_srv_rerror_rate', 'dst_host_srv_serror_rate',\n",
      "       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
      "       'outcome'],\n",
      "      dtype='object')\n",
      "Index(['hot', 'flag', 'land', 'count', 'urgent', 'service', 'duration',\n",
      "       'num_root', 'dst_bytes', 'logged_in', 'src_bytes', 'srv_count',\n",
      "       'num_shells', 'root_shell', 'rerror_rate', 'serror_rate',\n",
      "       'su_attempted', 'diff_srv_rate', 'protocol_type', 'same_srv_rate',\n",
      "       'dst_host_count', 'is_guest_login', 'wrong_fragment', 'num_compromised',\n",
      "       'srv_rerror_rate', 'srv_serror_rate', 'num_access_files',\n",
      "       'num_failed_logins', 'dst_host_srv_count', 'num_file_creations',\n",
      "       'srv_diff_host_rate', 'dst_host_rerror_rate', 'dst_host_serror_rate',\n",
      "       'dst_host_diff_srv_rate', 'dst_host_same_srv_rate',\n",
      "       'dst_host_srv_rerror_rate', 'dst_host_srv_serror_rate',\n",
      "       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
      "       'outcome'],\n",
      "      dtype='object')\n",
      "Folder './tmp/intrusion' already exists.\n"
     ]
    }
   ],
   "source": [
    "read_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54d60bf-c25d-4443-ae03-1065a4d28ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"intrusionpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM metadata_table_intrusion ORDER BY date DESC LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "        \n",
    "    folder_path = f\"version{version}\"\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"version{version}/X_train.npy\")\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_train = np.load(BytesIO(data))\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_test = np.load(BytesIO(data))\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/intrusion/models'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/intrusion/models/lrc.pkl', 'wb') as f:\n",
    "        pickle.dump(lrc, f)\n",
    "    s3_client.upload_file(\"tmp/intrusion/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/intrusion/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/intrusion/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "#     #Decision Tree\n",
    "#     start_train = time.time()\n",
    "#     dtc = DecisionTreeClassifier()\n",
    "#     dtc.fit(X_train, y_train)\n",
    "#     end_train = time.time()\n",
    "#     start_test = time.time()\n",
    "#     y_pred3=dtc.predict(X_test)\n",
    "#     end_test = time.time()\n",
    "#     accuracy = accuracy_score(y_test,y_pred3)\n",
    "#     f1 = f1_score(y_test, y_pred3)\n",
    "#     precision = precision_score(y_test, y_pred3)\n",
    "#     recall = recall_score(y_test, y_pred3)\n",
    "#     metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "#     with open('./tmp/intrusion/models/dtc.pkl', 'wb') as f:\n",
    "#         pickle.dump(dtc, f)\n",
    "#     s3_client.upload_file(\"tmp/intrusion/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Support Vector Machine\n",
    "#     start_train = time.time()\n",
    "#     svc = SVC()\n",
    "#     svc.fit(X_train, y_train)\n",
    "#     end_train = time.time()\n",
    "#     start_test = time.time()\n",
    "#     y_pred4=svc.predict(X_test)\n",
    "#     end_test = time.time()\n",
    "#     accuracy = accuracy_score(y_test,y_pred4)\n",
    "#     f1 = f1_score(y_test,y_pred4)\n",
    "#     precision = precision_score(y_test, y_pred4)\n",
    "#     recall = recall_score(y_test, y_pred4)\n",
    "#     metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "#     with open('./tmp/intrusion/models/svc.pkl', 'wb') as f:\n",
    "#         pickle.dump(svc, f)\n",
    "#     s3_client.upload_file(\"tmp/intrusion/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Gradient Boost\n",
    "#     start_train = time.time()\n",
    "#     gbc = GradientBoostingClassifier()\n",
    "#     gbc.fit(X_train, y_train)\n",
    "#     end_train = time.time()\n",
    "#     start_test = time.time()\n",
    "#     y_pred5=gbc.predict(X_test)\n",
    "#     end_test = time.time()\n",
    "#     accuracy = accuracy_score(y_test,y_pred5)\n",
    "#     f1 = f1_score(y_test, y_pred5)\n",
    "#     precision = precision_score(y_test, y_pred5)\n",
    "#     recall = (recall_score(y_test, y_pred5))\n",
    "#     metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "#     with open('./tmp/intrusion/models/gbc.pkl', 'wb') as f:\n",
    "#         pickle.dump(gbc, f)\n",
    "#     s3_client.upload_file(\"tmp/intrusion/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "#     #Gaussian Naive Bayes\n",
    "#     start_train = time.time()\n",
    "#     gnb = GaussianNB()\n",
    "#     gnb.fit(X_train, y_train)\n",
    "#     end_train = time.time()\n",
    "#     start_test = time.time()\n",
    "#     y_pred6=gnb.predict(X_test)\n",
    "#     end_test = time.time()\n",
    "#     accuracy = accuracy_score(y_test,y_pred6)\n",
    "#     f1 = f1_score(y_test, y_pred6)\n",
    "#     precision = precision_score(y_test,y_pred6)\n",
    "#     recall = recall_score(y_test, y_pred6)\n",
    "#     metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "#     with open('./tmp/intrusion/models/gnb.pkl', 'wb') as f:\n",
    "#         pickle.dump(gnb, f)      \n",
    "#     s3_client.upload_file(\"tmp/intrusion/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Artificial Neural Network\n",
    "#     input_shape = [X_train.shape[1]]\n",
    "#     start_train = time.time()\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "#         tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "#         tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.build()\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "#     history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "#     end_train=time.time()\n",
    "#     start_test = time.time()\n",
    "#     y_pred7 = model.predict(X_test)\n",
    "#     y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "#     end_test = time.time()\n",
    "#     print(y_pred7)\n",
    "#     accuracy = accuracy_score(y_test,y_pred7)\n",
    "#     f1 = f1_score(y_test, y_pred7)\n",
    "#     precision = precision_score(y_test,y_pred7)\n",
    "#     recall = recall_score(y_test, y_pred7)\n",
    "#     # accuracy = history.history['accuracy'][11]\n",
    "#     metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "#     with open('./tmp/intrusion/models/ann.pkl', 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "#     s3_client.upload_file(\"tmp/intrusion/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO intrusion_model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (name, version) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "        # Iterate through DataFrame rows and insert into the table\n",
    "        for index, row in metrics.iterrows():\n",
    "            cursor.execute(insert_query, (\n",
    "                row['Model'], \n",
    "                row['Version'], \n",
    "                f\"s3://intrusionpipeline/version{version}/{row['Model']}/model.pkl\", \n",
    "                False, \n",
    "                row['Accuracy'], \n",
    "                row['F1'], \n",
    "                row['Precision'], \n",
    "                row['Recall'], \n",
    "                row['Train_Time'], \n",
    "                row['Test_Time']\n",
    "            ))\n",
    "    \n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d607d1b4-6c68-4027-a69f-99ebf5c8892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully.\n",
      "version2/X_train.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './tmp/intrusion/models' already exists.\n",
      "Connected to PostgreSQL successfully.\n",
      "Data inserted successfully.\n",
      "PostgreSQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "train_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67e1108-cdba-4ec5-b6b3-c77a277bd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_deploy() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    import json\n",
    "    import os \n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import boto3\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM intrusion_model_metrics ORDER BY created_at desc LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    #print(f\"df = {df['version'][0]}\")\n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "        \n",
    "    #print(f\"version = {version}\")\n",
    "    \n",
    "    try:\n",
    "        fetch_query = f\"SELECT * FROM intrusion_model_metrics where version={version} order by accuracy desc limit 1;\"\n",
    "        model = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    \n",
    "    name = f\"{model['name'][0]}-version{version}-id\"\n",
    "    print(name)\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "    \n",
    "    isvc2 = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=name,\n",
    "            namespace=namespace,\n",
    "            annotations={'sidecar.istio.io/inject': 'false'}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=\"s3-service-account\",\n",
    "                sklearn=V1beta1SKLearnSpec(\n",
    "                    storage_uri=model['uri'][0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc2)\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM intrusion_model_metrics where in_use is true LIMIT 1;\"\n",
    "        old_model = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    update_query_new = \"\"\"\n",
    "        UPDATE intrusion_model_metrics\n",
    "        SET in_use = true\n",
    "        WHERE name = %s and version = %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    update_query_old = \"\"\"\n",
    "        UPDATE intrusion_model_metrics\n",
    "        SET in_use = false\n",
    "        WHERE name = %s and version = %s;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(update_query_new, (model['name'][0], int(model['version'][0])))\n",
    "        if(not old_model.empty):\n",
    "            cursor.execute(update_query_old, (old_model['name'][0], int(old_model['version'][0])))\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    if(not old_model.empty):\n",
    "        del_name = f\"{old_model['name'][0]}-version{old_model['version'][0]}-id\"\n",
    "        namespace = utils.get_default_target_namespace()\n",
    "\n",
    "        # Initialize the KServe client\n",
    "        KServe = KServeClient()\n",
    "\n",
    "        # Delete the inference service\n",
    "        KServe.delete(del_name, namespace)\n",
    "    \n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8290b526-ebc3-4393-8ab8-1eb88a81422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully.\n",
      "rfc-version2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_eval_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032eba75-37a3-4d0b-98dd-ec05706e9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "check_condition_op = components.func_to_container_op(func=check_condition, base_image='python:3.7', packages_to_install=['pandas==1.1.5', 'sqlalchemy==1.4.45', 'boto3', 'psycopg2-binary'])\n",
    "\n",
    "read_csv_op = components.func_to_container_op(func=read_file, output_component_file='preprocess.yaml', base_image='python:3.7', packages_to_install=['pandas==1.1.5','scikit-learn==1.0.1', 'kfp', 'numpy', 'minio', 'psycopg2-binary', 'sqlalchemy==1.4.45','boto3'])\n",
    "\n",
    "train_op = components.func_to_container_op(func=train_op, output_component_file='train.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow', 'psycopg2-binary', 'sqlalchemy','boto3'])\n",
    "\n",
    "eval_deploy = components.func_to_container_op(func=model_eval_deploy, output_component_file='eval_deploy.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow', 'psycopg2-binary', 'sqlalchemy','boto3','kubernetes','kserve'])\n",
    "\n",
    "read_data_op = kfp.components.load_component_from_file('preprocess.yaml')\n",
    "train_op = kfp.components.load_component_from_file('train.yaml')\n",
    "eval_deploy_op = kfp.components.load_component_from_file('eval_deploy.yaml')\n",
    "\n",
    "\n",
    "def ml_pipeline():\n",
    "    check_condition = check_condition_op()\n",
    "    with dsl.Condition(check_condition.output == 'True'):\n",
    "        preprocess = read_csv_op()\n",
    "        train = train_op().after(preprocess)\n",
    "        eval_deploy = eval_deploy_op().after(train)\n",
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(ml_pipeline, 'intrusion_pipeline.yaml', disable_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56647615-8ece-45ed-9b2f-e7e4d815a164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
