{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971d3696-218e-413b-b5b2-27a5756fb4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.0.1\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.21.6)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.8.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7dc5bd-a304-4212-aa0d-9660181b0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392f4dc2-4106-4d2c-a268-496cad17e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-2.0.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (3.0.3)\n",
      "Installing collected packages: sqlalchemy\n",
      "Successfully installed sqlalchemy-2.0.31\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e187e9b-db62-494a-bf72-ef3a945be782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a223a087-6a89-4b5f-95b1-ce5fd9b39a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(is_experiment: bool = False) -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    preprocess_df = {'version':1}\n",
    "    \n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "    def preprocess(df):\n",
    "        df = df.drop(columns=['name', 'md5'])\n",
    "        for i in df.columns:\n",
    "            if i != 'outcome':\n",
    "                df[i] = boxcox(df[i], 0.5)\n",
    "                zscore_normalization(df, i)\n",
    "        correlation_matrix = df.corr()\n",
    "        cols_to_drop = []\n",
    "        for i in df.columns:\n",
    "            for j in df.columns:\n",
    "                #drop columns with low correlation to target variable\n",
    "                if i != j and i != 'outcome' and j != 'outcome' and abs(correlation_matrix[i][j]) > 0.6 and i not in cols_to_drop and j not in cols_to_drop:\n",
    "                    cols_to_drop.append(i)\n",
    "        cols_to_drop = set(cols_to_drop)\n",
    "        for i in df.columns:\n",
    "            if i != 'outcome' and i in cols_to_drop:\n",
    "                preprocess_df[i] = None\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        df = df.dropna()\n",
    "        df = df.dropna(axis=1)\n",
    "        return df\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "            \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM malware_data WHERE outcome != 2;')\n",
    "            chunksize = 10000 \n",
    "\n",
    "            chunks = pd.read_sql_query(query, conn, chunksize=chunksize)\n",
    "\n",
    "            features_list = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                features_df = pd.json_normalize(chunk['features'])\n",
    "                features_df['outcome'] = chunk['outcome']\n",
    "                \n",
    "                df = pd.concat([df, features_df], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "\n",
    "    df = preprocess(df)\n",
    "    \n",
    "    X = df.drop(columns=['outcome'])\n",
    "    y = df['outcome']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    print(X_train.columns)\n",
    "    \n",
    "    bucket_name=\"malwarepipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    folder_path = './tmp/malware'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    df.to_csv(\"./tmp/malware/malware_data.csv\")\n",
    "    np.save(\"./tmp/malware/X_train.npy\",X_train)\n",
    "    np.save(\"./tmp/malware/y_train.npy\",y_train)\n",
    "    np.save(\"./tmp/malware/X_test.npy\",X_test)\n",
    "    np.save(\"./tmp/malware/y_test.npy\",y_test)\n",
    "    \n",
    "    \n",
    "    if(not is_experiment):\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                query = text('SELECT * FROM metadata_table_malware ORDER BY version DESC LIMIT 1;')\n",
    "                data = pd.read_sql_query(query, conn)\n",
    "                version = data['version'].iloc[0] + 1\n",
    "                print(version)\n",
    "        except Exception as e:\n",
    "            version = 1\n",
    "        \n",
    "        s3_client.upload_file(\"./tmp/malware/malware_data.csv\", bucket_name, f\"version{version}/malware_dataset.csv\")\n",
    "        s3_client.upload_file(\"./tmp/malware/X_train.npy\", bucket_name, f\"version{version}/X_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/malware/y_train.npy\", bucket_name, f\"version{version}/y_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/malware/X_test.npy\", bucket_name, f\"version{version}/X_test.npy\")\n",
    "        s3_client.upload_file(\"./tmp/malware/y_test.npy\", bucket_name, f\"version{version}/y_test.npy\")\n",
    "        \n",
    "        preprocess_df['version'] = version\n",
    "        mean_df = pd.DataFrame([preprocess_df])\n",
    "        meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), len(X.columns), json.dumps(df.dtypes.astype(str).to_dict()),mean_df.iloc[0].to_json()]], columns = ['version', 'date', 'features', 'types','factor'])\n",
    "        meta_df.to_sql(\"metadata_table_malware\", engine, if_exists='append', index=False)\n",
    "    else:\n",
    "        s3_client.upload_file(\"./tmp/malware/malware_data.csv\", bucket_name, f\"experiment/malware_dataset.csv\")\n",
    "        s3_client.upload_file(\"./tmp/malware/X_train.npy\", bucket_name, f\"experiment/X_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/malware/y_train.npy\", bucket_name, f\"experiment/y_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/malware/X_test.npy\", bucket_name, f\"experiment/X_test.npy\")\n",
    "        s3_client.upload_file(\"./tmp/malware/y_test.npy\", bucket_name, f\"experiment/y_test.npy\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b79d773-c370-40fb-9e58-4bcba9dbe02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(is_experiment: bool = False) -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"malwarepipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    if(not is_experiment):\n",
    "        db_details = {\n",
    "            'dbname': db,\n",
    "            'user': user,\n",
    "            'password': pswd,\n",
    "            'host': host,\n",
    "            'port': port\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # Connect to PostgreSQL\n",
    "        try:\n",
    "            conn = psycopg2.connect(**db_details)\n",
    "            cursor = conn.cursor()\n",
    "            print(\"Connected to PostgreSQL successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "            exit()\n",
    "\n",
    "        # Query to fetch data from the table\n",
    "        try:\n",
    "            fetch_query = \"SELECT * FROM metadata_table_malware ORDER BY date DESC LIMIT 1;\"\n",
    "            df = pd.read_sql(fetch_query, conn)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "        if(not df.empty):\n",
    "            version = df['version'][0]\n",
    "        else:\n",
    "            version = 1\n",
    "\n",
    "        folder_path = f\"version{version}\"\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"version{version}/X_train.npy\")\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_train = np.load(BytesIO(data))\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_train = np.load(BytesIO(data))\n",
    "\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_test = np.load(BytesIO(data))\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_test = np.load(BytesIO(data))\n",
    "\n",
    "    else:\n",
    "        version = 0\n",
    "        folder_path = 'experiment'\n",
    "        \n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/X_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_train = np.load(BytesIO(data))\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/y_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_train = np.load(BytesIO(data))\n",
    "\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/X_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_test = np.load(BytesIO(data))\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/y_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_test = np.load(BytesIO(data))\n",
    "        \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/malware/models'\n",
    "\n",
    "\n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/lrc.pkl', 'wb') as f:\n",
    "        pickle.dump(lrc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3)\n",
    "    precision = precision_score(y_test, y_pred3)\n",
    "    recall = recall_score(y_test, y_pred3)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/dtc.pkl', 'wb') as f:\n",
    "        pickle.dump(dtc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Support Vector Machine\n",
    "    start_train = time.time()\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred4=svc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred4)\n",
    "    f1 = f1_score(y_test,y_pred4)\n",
    "    precision = precision_score(y_test, y_pred4)\n",
    "    recall = recall_score(y_test, y_pred4)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/svc.pkl', 'wb') as f:\n",
    "        pickle.dump(svc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Gradient Boost\n",
    "    start_train = time.time()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred5=gbc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5)\n",
    "    precision = precision_score(y_test, y_pred5)\n",
    "    recall = (recall_score(y_test, y_pred5))\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/gbc.pkl', 'wb') as f:\n",
    "        pickle.dump(gbc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    start_train = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred6=gnb.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred6)\n",
    "    f1 = f1_score(y_test, y_pred6)\n",
    "    precision = precision_score(y_test,y_pred6)\n",
    "    recall = recall_score(y_test, y_pred6)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "    with open('./tmp/malware/models/gnb.pkl', 'wb') as f:\n",
    "        pickle.dump(gnb, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Artificial Neural Network\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    start_train = time.time()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.build()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "    end_train=time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred7 = model.predict(X_test)\n",
    "    y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "    end_test = time.time()\n",
    "    print(y_pred7)\n",
    "    accuracy = accuracy_score(y_test,y_pred7)\n",
    "    f1 = f1_score(y_test, y_pred7)\n",
    "    precision = precision_score(y_test,y_pred7)\n",
    "    recall = recall_score(y_test, y_pred7)\n",
    "    # accuracy = history.history['accuracy'][11]\n",
    "    metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "    with open('./tmp/malware/models/ann.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "\n",
    "    if(not is_experiment):\n",
    "        db_details = {\n",
    "            'dbname': db,\n",
    "            'user': user,\n",
    "            'password': pswd,\n",
    "            'host': host,\n",
    "            'port': port\n",
    "        }\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO malware_model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (name, version) DO NOTHING;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = psycopg2.connect(**db_details)\n",
    "            cursor = conn.cursor()\n",
    "            print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "            # Iterate through DataFrame rows and insert into the table\n",
    "            for index, row in metrics.iterrows():\n",
    "                cursor.execute(insert_query, (\n",
    "                    row['Model'], \n",
    "                    row['Version'], \n",
    "                    f\"s3://malwarepipeline/version{version}/{row['Model']}/model.pkl\", \n",
    "                    False, \n",
    "                    row['Accuracy'], \n",
    "                    row['F1'], \n",
    "                    row['Precision'], \n",
    "                    row['Recall'], \n",
    "                    row['Train_Time'], \n",
    "                    row['Test_Time']\n",
    "                ))\n",
    "\n",
    "            conn.commit()\n",
    "            print(\"Data inserted successfully.\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection closed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    else:\n",
    "        print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebdfa4b-504b-4833-ba56-e711d7f3684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_functions() -> None:\n",
    "    read_file(True)\n",
    "    train_op(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c52071-fea1-43f4-9935-0cfb5c2e8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['checksum', 'exportnb', 'imagebase', 'subsystem', 'baseofcode',\n",
      "       'sectionsnb', 'sizeofcode', 'resourcesnb', 'importsnbdll',\n",
      "       'sizeofheaders', 'importsnbordinal', 'resourcesminsize',\n",
      "       'sectionalignment', 'minorimageversion', 'resourcesmeansize',\n",
      "       'sizeofheapreserve', 'sizeofstackcommit', 'dllcharacteristics',\n",
      "       'minorlinkerversion', 'sizeofstackreserve', 'addressofentrypoint',\n",
      "       'resourcesminentropy', 'sectionsmeanentropy', 'sectionsmeanrawsize',\n",
      "       'resourcesmeanentropy', 'sizeofoptionalheader', 'loadconfigurationsize',\n",
      "       'majorsubsystemversion', 'sizeofinitializeddata',\n",
      "       'sectionsminvirtualsize', 'versioninformationsize',\n",
      "       'sectionsmeanvirtualsize', 'sizeofuninitializeddata',\n",
      "       'majoroperatingsystemversion', 'minoroperatingsystemversion'],\n",
      "      dtype='object')\n",
      "Folder './tmp/malware' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 18:22:47.375191: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 18:22:47.577375: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2024-07-30 18:22:47.577485: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2024-07-30 18:22:47.604227: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './tmp/malware/models' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 18:26:54.052313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 18:26:54.053292: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "432/432 [==============================] - 12s 26ms/step - loss: 0.0862 - accuracy: 0.9751 - val_loss: 0.0471 - val_accuracy: 0.9836\n",
      "Epoch 2/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0436 - accuracy: 0.9852 - val_loss: 0.0415 - val_accuracy: 0.9863\n",
      "Epoch 3/25\n",
      "432/432 [==============================] - 11s 24ms/step - loss: 0.0387 - accuracy: 0.9867 - val_loss: 0.0388 - val_accuracy: 0.9866\n",
      "Epoch 4/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0361 - accuracy: 0.9876 - val_loss: 0.0364 - val_accuracy: 0.9876\n",
      "Epoch 5/25\n",
      "432/432 [==============================] - 11s 24ms/step - loss: 0.0336 - accuracy: 0.9885 - val_loss: 0.0349 - val_accuracy: 0.9885\n",
      "Epoch 6/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0318 - accuracy: 0.9891 - val_loss: 0.0341 - val_accuracy: 0.9891\n",
      "Epoch 7/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0307 - accuracy: 0.9896 - val_loss: 0.0326 - val_accuracy: 0.9900\n",
      "Epoch 8/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.0328 - val_accuracy: 0.9894\n",
      "Epoch 9/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.0304 - val_accuracy: 0.9905\n",
      "Epoch 10/25\n",
      "432/432 [==============================] - 11s 24ms/step - loss: 0.0273 - accuracy: 0.9907 - val_loss: 0.0320 - val_accuracy: 0.9899\n",
      "Epoch 11/25\n",
      "432/432 [==============================] - 11s 24ms/step - loss: 0.0263 - accuracy: 0.9909 - val_loss: 0.0294 - val_accuracy: 0.9909\n",
      "Epoch 12/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 0.0311 - val_accuracy: 0.9898\n",
      "Epoch 13/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.0288 - val_accuracy: 0.9908\n",
      "Epoch 14/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.0288 - val_accuracy: 0.9912\n",
      "Epoch 15/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 0.0292 - val_accuracy: 0.9908\n",
      "Epoch 16/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0229 - accuracy: 0.9921 - val_loss: 0.0294 - val_accuracy: 0.9908\n",
      "Epoch 17/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.0290 - val_accuracy: 0.9909\n",
      "Epoch 18/25\n",
      "432/432 [==============================] - 12s 27ms/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.0274 - val_accuracy: 0.9917\n",
      "Epoch 19/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0310 - val_accuracy: 0.9903\n",
      "Epoch 20/25\n",
      "432/432 [==============================] - 12s 27ms/step - loss: 0.0208 - accuracy: 0.9927 - val_loss: 0.0275 - val_accuracy: 0.9913\n",
      "Epoch 21/25\n",
      "432/432 [==============================] - 11s 24ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 0.0293 - val_accuracy: 0.9917\n",
      "Epoch 22/25\n",
      "432/432 [==============================] - 10s 24ms/step - loss: 0.0200 - accuracy: 0.9930 - val_loss: 0.0285 - val_accuracy: 0.9909\n",
      "Epoch 23/25\n",
      "432/432 [==============================] - 11s 24ms/step - loss: 0.0198 - accuracy: 0.9929 - val_loss: 0.0280 - val_accuracy: 0.9913\n",
      "Epoch 24/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.0283 - val_accuracy: 0.9915\n",
      "Epoch 25/25\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.0276 - val_accuracy: 0.9915\n",
      "863/863 [==============================] - 9s 10ms/step\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "INFO:tensorflow:Assets written to: ram://51a67108-3363-4cb9-baef-99a53ae36d42/assets\n",
      "  Version Model  Accuracy        F1  Precision    Recall  Train_Time  \\\n",
      "0       0   lrc  0.976277  0.960117   0.970577  0.949880   19.742287   \n",
      "1       0   rfc  0.994603  0.991019   0.991557  0.990482   25.545087   \n",
      "2       0   dtc  0.991271  0.985478   0.985775  0.985181    3.139079   \n",
      "3       0   svc  0.988989  0.981673   0.982384  0.980964  124.110386   \n",
      "4       0   gbc  0.991054  0.985098   0.986586  0.983614   52.981393   \n",
      "5       0   gnb  0.969142  0.950064   0.925017  0.976506    0.067562   \n",
      "6       0   ann  0.991489  0.985849   0.985434  0.986265    9.719096   \n",
      "\n",
      "   Test_Time  \n",
      "0   0.005082  \n",
      "1   0.599747  \n",
      "2   0.047371  \n",
      "3  14.333425  \n",
      "4   0.080704  \n",
      "5   0.010349  \n",
      "6   0.000000  \n"
     ]
    }
   ],
   "source": [
    "run_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ff0ac-6e07-4183-b715-fd7ad5e52665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
