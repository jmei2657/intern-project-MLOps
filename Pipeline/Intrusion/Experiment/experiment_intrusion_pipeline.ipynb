{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e74e1-0dea-4389-9f2f-58d1e07a51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.0.1\n",
    "!pip install pandas \n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy\n",
    "!pip install kfp numpy\n",
    "\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import time\n",
    "import os\n",
    "\n",
    "from kfp import components\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7184db3-a366-4456-8b9c-e2928502a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    #Dictionary to save mean, sd, and, encoder\n",
    "    # preprocess_df = {'version':1}\n",
    "    \n",
    "    #Perform normalization\n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        # preprocess_df[name] = (mean, sd)\n",
    "        \n",
    "    #Encode text \n",
    "    def encode_text(df, name):\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "        enc = OrdinalEncoder()\n",
    "        data = enc.fit_transform(df[name].values.reshape(-1,1))\n",
    "        df[name] = data.flatten()\n",
    "        # preprocess_df[name + '_encoder'] = pickle.dumps(enc)\n",
    "        \n",
    "    #Data preprocessing\n",
    "    def preprocess(df):\n",
    "        df.columns = [\n",
    "            'duration',\n",
    "            'protocol_type',\n",
    "            'service',\n",
    "            'flag',\n",
    "            'src_bytes',\n",
    "            'dst_bytes',\n",
    "            'land',\n",
    "            'wrong_fragment',\n",
    "            'urgent',\n",
    "            'hot',\n",
    "            'num_failed_logins',\n",
    "            'logged_in',\n",
    "            'num_compromised',\n",
    "            'root_shell',\n",
    "            'su_attempted',\n",
    "            'num_root',\n",
    "            'num_file_creations',\n",
    "            'num_shells',\n",
    "            'num_access_files',\n",
    "            'num_outbound_cmds',\n",
    "            'is_host_login',\n",
    "            'is_guest_login',\n",
    "            'count',\n",
    "            'srv_count',\n",
    "            'serror_rate',\n",
    "            'srv_serror_rate',\n",
    "            'rerror_rate',\n",
    "            'srv_rerror_rate',\n",
    "            'same_srv_rate',\n",
    "            'diff_srv_rate',\n",
    "            'srv_diff_host_rate',\n",
    "            'dst_host_count',\n",
    "            'dst_host_srv_count',\n",
    "            'dst_host_same_srv_rate',\n",
    "            'dst_host_diff_srv_rate',\n",
    "            'dst_host_same_src_port_rate',\n",
    "            'dst_host_srv_diff_host_rate',\n",
    "            'dst_host_serror_rate',\n",
    "            'dst_host_srv_serror_rate',\n",
    "            'dst_host_rerror_rate',\n",
    "            'dst_host_srv_rerror_rate',\n",
    "            'outcome'\n",
    "        ]\n",
    "        for col in df.columns:\n",
    "            t = (df[col].dtype)\n",
    "            if col != 'outcome':\n",
    "                if (t == int or t == float):\n",
    "                    zscore_normalization(df, col)\n",
    "               \n",
    "                else:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    encode_text(df, col)\n",
    "                            \n",
    "        for col in df.columns:\n",
    "            if len(df[col].unique()) == 1:\n",
    "                df.drop(col, inplace=True,axis=1)\n",
    "\n",
    "        df.loc[df['outcome'] != \"normal.\", 'outcome']  = 1\n",
    "        df.loc[df['outcome'] == \"normal.\", 'outcome']  = 0\n",
    "        encode_text(df, \"outcome\")\n",
    "        correlation = df.corrwith(df[\"outcome\"])\n",
    "      \n",
    "        \n",
    "        row = 0 \n",
    "        for num in correlation:\n",
    "            if num >= -0.05 and num <= 0.05:\n",
    "                df.drop(df.columns[row], axis=1, inplace=True)\n",
    "                row += 1\n",
    "        return df\n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    batch_size = 10000\n",
    "    fetch_query = \"SELECT * FROM intrusion_data where outcome != 2;\"\n",
    "            \n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "        return None\n",
    "        \n",
    "    print(df.columns)\n",
    "    \n",
    "    df = df.drop(columns=['timestamp','uid'])\n",
    "    df = preprocess(df)\n",
    "    X = df.drop(columns=['outcome'])\n",
    "    y = df['outcome']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    bucket_name=\"intrusion-pipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    folder_path = './tmp/intrusion'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "\n",
    "    df.to_csv(\"./tmp/intrusion/intrusion_data.csv\")\n",
    "    s3_client.upload_file(\"./tmp/intrusion/intrusion_data.csv\", bucket_name, \"intrusion_dataset.csv\")\n",
    "    np.save(\"./tmp/intrusion/X_train.npy\",X_train)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/X_train.npy\", bucket_name, \"X_train.npy\")\n",
    "    np.save(\"./tmp/intrusion/y_train.npy\",y_train)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/y_train.npy\", bucket_name, \"y_train.npy\")\n",
    "    np.save(\"./tmp/intrusion/X_test.npy\",X_test)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/X_test.npy\", bucket_name, \"X_test.npy\")\n",
    "    np.save(\"./tmp/intrusion/y_test.npy\",y_test)\n",
    "    s3_client.upload_file(\"./tmp/intrusion/y_test.npy\", bucket_name, \"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bda48-0e64-4c2a-ada2-20eadef73a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"intrusion-pipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"X_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"y_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"X_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_test = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"y_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/intrusion/models'\n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    # version = 1\n",
    "    # folder_path = f\"version{version}\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    # with open('./tmp/intrusion/models/lrc.pkl', 'wb') as f:\n",
    "    #     pickle.dump(lrc, f)\n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    # with open('./tmp/intrusion/models/rfc.pkl', 'wb') as f:\n",
    "    #     pickle.dump(rfc, f)\n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3)\n",
    "    precision = precision_score(y_test, y_pred3)\n",
    "    recall = recall_score(y_test, y_pred3)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    # with open('./tmp/intrusion/models/dtc.pkl', 'wb') as f:\n",
    "    #     pickle.dump(dtc, f)\n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Support Vector Machine\n",
    "    start_train = time.time()\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred4=svc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred4)\n",
    "    f1 = f1_score(y_test,y_pred4)\n",
    "    precision = precision_score(y_test, y_pred4)\n",
    "    recall = recall_score(y_test, y_pred4)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    # with open('./tmp/intrusion/models/svc.pkl', 'wb') as f:\n",
    "    #     pickle.dump(svc, f)\n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Gradient Boost\n",
    "    start_train = time.time()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred5=gbc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5)\n",
    "    precision = precision_score(y_test, y_pred5)\n",
    "    recall = (recall_score(y_test, y_pred5))\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    # with open('./tmp/intrusion/models/gbc.pkl', 'wb') as f:\n",
    "    #     pickle.dump(gbc, f)\n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    start_train = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred6=gnb.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred6)\n",
    "    f1 = f1_score(y_test, y_pred6)\n",
    "    precision = precision_score(y_test,y_pred6)\n",
    "    recall = recall_score(y_test, y_pred6)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "    # with open('./tmp/intrusion/models/gnb.pkl', 'wb') as f:\n",
    "    #     pickle.dump(gnb, f)      \n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Artificial Neural Network\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    start_train = time.time()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.build()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "    end_train=time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred7 = model.predict(X_test)\n",
    "    y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "    end_test = time.time()\n",
    "    print(y_pred7)\n",
    "    accuracy = accuracy_score(y_test,y_pred7)\n",
    "    f1 = f1_score(y_test, y_pred7)\n",
    "    precision = precision_score(y_test,y_pred7)\n",
    "    recall = recall_score(y_test, y_pred7)\n",
    "    # accuracy = history.history['accuracy'][11]\n",
    "    metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "    # with open('./tmp/intrusion/models/ann.pkl', 'wb') as f:\n",
    "    #     pickle.dump(model, f)\n",
    "    # s3_client.upload_file(\"tmp/intrusion/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "    \n",
    "    print(metrics)\n",
    "\n",
    "#     db_details = {\n",
    "#         'dbname': db,\n",
    "#         'user': user,\n",
    "#         'password': pswd,\n",
    "#         'host': host,\n",
    "#         'port': port\n",
    "#     }\n",
    "        \n",
    "#     insert_query = \"\"\"\n",
    "#         INSERT INTO model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "#         VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "#         ON CONFLICT (name, version) DO NOTHING;\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(**db_details)\n",
    "#         cursor = conn.cursor()\n",
    "#         print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "#         # Iterate through DataFrame rows and insert into the table\n",
    "#         for index, row in metrics.iterrows():\n",
    "#             cursor.execute(insert_query, (\n",
    "#                 row['Model'], \n",
    "#                 row['Version'], \n",
    "#                 f\"s3://mlpipelineews/version{version}/{row['Model']}/model.pkl\", \n",
    "#                 False, \n",
    "#                 row['Accuracy'], \n",
    "#                 row['F1'], \n",
    "#                 row['Precision'], \n",
    "#                 row['Recall'], \n",
    "#                 row['Train_Time'], \n",
    "#                 row['Test_Time']\n",
    "#             ))\n",
    "    \n",
    "#         conn.commit()\n",
    "#         print(\"Data inserted successfully.\")\n",
    "\n",
    "#         cursor.close()\n",
    "#         conn.close()\n",
    "#         print(\"PostgreSQL connection closed.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d859c2-c285-456f-991d-db676f04932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "read_csv_op = components.func_to_container_op(func=read_file, output_component_file='preprocess.yaml', base_image='python:3.7', packages_to_install=['pandas','scikit-learn==1.0.1', 'kfp', 'numpy', 'minio'])\n",
    "\n",
    "train_op = components.func_to_container_op(func=train_op, output_component_file='train.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow'])\n",
    "\n",
    "read_data_op = kfp.components.load_component_from_file('preprocess.yaml')\n",
    "train_op = kfp.components.load_component_from_file('train.yaml')\n",
    "\n",
    "\n",
    "def ml_pipeline():\n",
    "    preprocess = read_csv_op()\n",
    "    train = train_op().after(preprocess)\n",
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(ml_pipeline, 'intrusion_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
