{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5bea247-ebaf-498f-bfb9-159eb51028dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.0.1\n",
      "  Downloading scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.21.6)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfecb77-b05f-42fe-9b11-74f8ecc49aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.6.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.3/614.3 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, greenlet, sqlalchemy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/usr/local/include/python3.9/greenlet'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7d4637-d0d7-4aa3-a708-a6916bcf0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-2.0.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (3.0.3)\n",
      "Installing collected packages: sqlalchemy\n",
      "Successfully installed sqlalchemy-2.0.31\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b090285-fb5f-4382-b874-1d3c0b16a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 16:34:40.064760: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 16:34:40.256427: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2024-07-10 16:34:40.256575: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2024-07-10 16:34:40.289059: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn kfp numpy\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import time\n",
    "import os\n",
    "\n",
    "from kfp import components\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c745793-3bb4-4c11-91a4-528354daa531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    preprocess_df = {'version':1}\n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "    def preprocess(df):\n",
    "        df = df.drop(columns=['Name', 'md5'])\n",
    "        for i in df.columns:\n",
    "            if i != 'legitimate':\n",
    "                #convert data to fit normal distribution\n",
    "                df[i] = boxcox(df[i], 0.5)\n",
    "                #normalize all numerical columns\n",
    "                zscore_normalization(df, i)\n",
    "        correlation_matrix = df.corr()\n",
    "        cols_to_drop = []\n",
    "        for i in df.columns:\n",
    "            for j in df.columns:\n",
    "                #drop columns with low correlation to target variable\n",
    "                if i != j and i != 'legitimate' and j != 'legitimate' and abs(correlation_matrix[i][j]) > 0.6 and i not in cols_to_drop and j not in cols_to_drop:\n",
    "                    cols_to_drop.append(i)\n",
    "        cols_to_drop = set(cols_to_drop)\n",
    "        for i in df.columns:\n",
    "            if i != 'legitimate' and i in cols_to_drop:\n",
    "                preprocess_df[i] = None\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        return df\n",
    "\n",
    "    file_path = 'https://raw.githubusercontent.com/tsimhadri-ews/internproject/malware-detection-0/src/MalwareData.csv'\n",
    "    df = pd.read_csv(file_path, sep='|')\n",
    "    df = preprocess(df)\n",
    "    X = df.drop(columns=['legitimate'])\n",
    "    y = df['legitimate']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "   \n",
    "    \n",
    "    bucket_name=\"mlpipelineews\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    folder_path = './tmp/malware'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "\n",
    "    df.to_csv(\"./tmp/malware/malware_data.csv\")\n",
    "    s3_client.upload_file(\"./tmp/malware/malware_data.csv\", bucket_name, \"malware_dataset.csv\")\n",
    "    np.save(\"./tmp/malware/X_train.npy\",X_train)\n",
    "    s3_client.upload_file(\"./tmp/malware/X_train.npy\", bucket_name, \"X_train.npy\")\n",
    "    np.save(\"./tmp/malware/y_train.npy\",y_train)\n",
    "    s3_client.upload_file(\"./tmp/malware/y_train.npy\", bucket_name, \"y_train.npy\")\n",
    "    np.save(\"./tmp/malware/X_test.npy\",X_test)\n",
    "    s3_client.upload_file(\"./tmp/malware/X_test.npy\", bucket_name, \"X_test.npy\")\n",
    "    np.save(\"./tmp/malware/y_test.npy\",y_test)\n",
    "    s3_client.upload_file(\"./tmp/malware/y_test.npy\", bucket_name, \"y_test.npy\")\n",
    "    \n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn: \n",
    "            query = text('SELECT * FROM metadata_table ORDER BY version DESC LIMIT 1')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            version = data['version'].iloc[0] + 1\n",
    "    except Exception as e:\n",
    "        version = 1\n",
    "    meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), len(X.columns), json.dumps(df.dtypes.astype(str).to_dict())]], columns = ['version', 'date', 'features', 'types'])\n",
    "    meta_df.to_sql(\"metadata_table\", engine, if_exists='append', index=False)\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn: \n",
    "            query = text('SELECT * FROM preprocess_table ORDER BY version DESC LIMIT 1')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            version = data['version'].iloc[0] + 1\n",
    "    except Exception as e:\n",
    "        version = 1\n",
    "        \n",
    "    preprocess_df['version'] = version\n",
    "    mean_df = pd.DataFrame([preprocess_df])\n",
    "    mean_df.to_sql(\"preprocess_table\", engine, if_exists='append', index=False)\n",
    "    # Insert training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a40a1-e4c8-4f09-a371-2363b9054ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './tmp/malware' already exists.\n",
      "placing into database\n",
      "Index(['sizeofoptionalheader', 'minorlinkerversion', 'sizeofcode',\n",
      "       'sizeofinitializeddata', 'sizeofuninitializeddata', 'baseofcode',\n",
      "       'baseofdata', 'imagebase', 'sectionalignment',\n",
      "       'majoroperatingsystemversion', 'minorimageversion',\n",
      "       'majorsubsystemversion', 'sizeofheaders', 'checksum', 'subsystem',\n",
      "       'dllcharacteristics', 'sizeofstackreserve', 'sizeofstackcommit',\n",
      "       'sizeofheapreserve', 'numberofrvaandsizes', 'sectionsnb',\n",
      "       'sectionsminentropy', 'sectionsmaxentropy', 'sectionmaxrawsize',\n",
      "       'sectionsminvirtualsize', 'sectionmaxvirtualsize', 'importsnb',\n",
      "       'importsnbordinal', 'exportnb', 'resourcesnb', 'resourcesminentropy',\n",
      "       'resourcesmaxentropy', 'resourcesminsize', 'resourcesmaxsize',\n",
      "       'loadconfigurationsize', 'versioninformationsize', 'uid', 'datatype'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "read_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c9aec5-58ac-4c99-be6c-cf11521f3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureStoreToDatabase() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "    \n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    preprocess_df = {'version':1}\n",
    "    \n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "    def preprocess(df):\n",
    "        df = df.drop(columns=['Name', 'md5'])\n",
    "        for i in df.columns:\n",
    "            if i != 'legitimate':\n",
    "                #convert data to fit normal distribution\n",
    "                df[i] = boxcox(df[i], 0.5)\n",
    "                #normalize all numerical columns\n",
    "                zscore_normalization(df, i)\n",
    "        correlation_matrix = df.corr()\n",
    "        cols_to_drop = []\n",
    "        for i in df.columns:\n",
    "            for j in df.columns:\n",
    "                #drop columns with low correlation to target variable\n",
    "                if i != j and i != 'legitimate' and j != 'legitimate' and abs(correlation_matrix[i][j]) > 0.6 and i not in cols_to_drop and j not in cols_to_drop:\n",
    "                    cols_to_drop.append(i)\n",
    "        cols_to_drop = set(cols_to_drop)\n",
    "        for i in df.columns:\n",
    "            if i != 'legitimate' and i in cols_to_drop:\n",
    "                preprocess_df[i] = None\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        return df\n",
    "\n",
    "    file_path = 'https://raw.githubusercontent.com/tsimhadri-ews/internproject/malware-detection-0/src/MalwareData.csv'\n",
    "    df = pd.read_csv(file_path, sep='|')\n",
    "    raw_df = df\n",
    "    df = preprocess(df) \n",
    "   \n",
    "    raw_df = raw_df[df.columns]\n",
    "    raw_x = raw_df.drop(columns=['legitimate'])\n",
    "    raw_y = raw_df['legitimate']\n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "\n",
    "    raw_x['uid'] = range(1, len(raw_x) + 1)\n",
    "    raw_x['datatype'] = 0\n",
    "    \n",
    "    raw_y = pd.DataFrame({'uid': raw_x['uid'], 'outcome': raw_y})\n",
    "    \n",
    "    raw_x.columns = raw_x.columns.str.lower()\n",
    "    raw_y.columns = raw_y.columns.str.lower()\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "    \n",
    "    print(\"placing into database\")\n",
    "    \n",
    "    raw_x.to_sql(\"malware_data\", engine, if_exists='append', index=False)\n",
    "    raw_y.to_sql(\"outcomes\", engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9af312-d7bd-44e0-b6ff-8f04dc35b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placing into database\n"
     ]
    }
   ],
   "source": [
    "FeatureStoreToDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539e60bd-306e-44b5-8f88-0dd28576c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"mlpipelineews\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"X_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"y_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"X_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_test = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"y_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/malware/models'\n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    version = 1\n",
    "    folder_path = f\"version{version}\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/lrc.pkl', 'wb') as f:\n",
    "        pickle.dump(lrc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3)\n",
    "    precision = precision_score(y_test, y_pred3)\n",
    "    recall = recall_score(y_test, y_pred3)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/dtc.pkl', 'wb') as f:\n",
    "        pickle.dump(dtc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Support Vector Machine\n",
    "    start_train = time.time()\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred4=svc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred4)\n",
    "    f1 = f1_score(y_test,y_pred4)\n",
    "    precision = precision_score(y_test, y_pred4)\n",
    "    recall = recall_score(y_test, y_pred4)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/svc.pkl', 'wb') as f:\n",
    "        pickle.dump(svc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Gradient Boost\n",
    "    start_train = time.time()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred5=gbc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5)\n",
    "    precision = precision_score(y_test, y_pred5)\n",
    "    recall = (recall_score(y_test, y_pred5))\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/gbc.pkl', 'wb') as f:\n",
    "        pickle.dump(gbc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    start_train = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred6=gnb.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred6)\n",
    "    f1 = f1_score(y_test, y_pred6)\n",
    "    precision = precision_score(y_test,y_pred6)\n",
    "    recall = recall_score(y_test, y_pred6)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "    with open('./tmp/malware/models/gnb.pkl', 'wb') as f:\n",
    "        pickle.dump(gnb, f)      \n",
    "    s3_client.upload_file(\"tmp/malware/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Artificial Neural Network\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    start_train = time.time()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.build()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "    end_train=time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred7 = model.predict(X_test)\n",
    "    y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "    end_test = time.time()\n",
    "    print(y_pred7)\n",
    "    accuracy = accuracy_score(y_test,y_pred7)\n",
    "    f1 = f1_score(y_test, y_pred7)\n",
    "    precision = precision_score(y_test,y_pred7)\n",
    "    recall = recall_score(y_test, y_pred7)\n",
    "    # accuracy = history.history['accuracy'][11]\n",
    "    metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "    with open('./tmp/malware/models/ann.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (name, version) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "        # Iterate through DataFrame rows and insert into the table\n",
    "        for index, row in metrics.iterrows():\n",
    "            cursor.execute(insert_query, (\n",
    "                row['Model'], \n",
    "                row['Version'], \n",
    "                f\"s3://mlpipelineews/version{version}/{row['Model']}/model.pkl\", \n",
    "                False, \n",
    "                row['Accuracy'], \n",
    "                row['F1'], \n",
    "                row['Precision'], \n",
    "                row['Recall'], \n",
    "                row['Train_Time'], \n",
    "                row['Test_Time']\n",
    "            ))\n",
    "    \n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81978865-f711-44e5-bd89-70a8b285eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './tmp/malware/models' already exists.\n",
      "Connected to PostgreSQL successfully.\n",
      "Data inserted successfully.\n",
      "PostgreSQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "train_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea987de7-5772-4da3-a76d-803225b2e994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
