{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd8b3a7-a5af-4dd1-a767-7af3f6f22bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:15:08.431360: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 19:15:08.576590: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2024-07-09 19:15:08.576738: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2024-07-09 19:15:08.652795: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn kfp numpy\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import time\n",
    "import os\n",
    "\n",
    "from kfp import components\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb70f6b-3df3-4d03-a1a6-d3cd96e2c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "    def preprocess(df):\n",
    "        #df = df.drop(columns=['Name', 'md5'])\n",
    "        for i in df.columns:\n",
    "            if i != 'outcome':\n",
    "                #convert data to fit normal distribution\n",
    "                df[i] = boxcox(df[i], 0.5)\n",
    "                #normalize all numerical columns\n",
    "                zscore_normalization(df, i)\n",
    "        correlation_matrix = df.corr()\n",
    "        cols_to_drop = []\n",
    "        for i in df.columns:\n",
    "            for j in df.columns:\n",
    "                #drop columns with low correlation to target variable\n",
    "                if i != j and i != 'outcome' and j != 'outcome' and abs(correlation_matrix[i][j]) > 0.6 and i not in cols_to_drop and j not in cols_to_drop:\n",
    "                    cols_to_drop.append(i)\n",
    "        cols_to_drop = set(cols_to_drop)\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        return df\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM outcomes join malware_data on malware_data.uid::text=outcomes.uid::text\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    cols = [col for col in df.columns if col != 'uid']\n",
    "    \n",
    "    #print(cols)\n",
    "    df = df[cols]\n",
    "    df = df.drop(columns=['datatype'])\n",
    "    df = preprocess(df)\n",
    "    \n",
    "    \n",
    "    X = df.drop(columns=['outcome'])\n",
    "    y = df['outcome']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    bucket_name=\"mlpipelineews\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    folder_path = './tmp/malware'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "\n",
    "    df.to_csv(\"./tmp/malware/malware_data.csv\")\n",
    "    s3_client.upload_file(\"./tmp/malware/malware_data.csv\", bucket_name, \"malware_dataset.csv\")\n",
    "    np.save(\"./tmp/malware/X_train.npy\",X_train)\n",
    "    s3_client.upload_file(\"./tmp/malware/X_train.npy\", bucket_name, \"X_train.npy\")\n",
    "    np.save(\"./tmp/malware/y_train.npy\",y_train)\n",
    "    s3_client.upload_file(\"./tmp/malware/y_train.npy\", bucket_name, \"y_train.npy\")\n",
    "    np.save(\"./tmp/malware/X_test.npy\",X_test)\n",
    "    s3_client.upload_file(\"./tmp/malware/X_test.npy\", bucket_name, \"X_test.npy\")\n",
    "    np.save(\"./tmp/malware/y_test.npy\",y_test)\n",
    "    s3_client.upload_file(\"./tmp/malware/y_test.npy\", bucket_name, \"y_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f154d29e-c0e1-4f31-8109-5afbc890150a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './tmp/malware' already exists.\n"
     ]
    }
   ],
   "source": [
    "read_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86768e8-3fbf-492a-8d9d-c176bd9ff8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"mlpipelineews\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"X_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"y_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"X_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_test = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=\"y_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/malware/models'\n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "        \n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM model_metrics ORDER BY created_at DESC LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'] + 1\n",
    "    else:\n",
    "        version = 1\n",
    "        \n",
    "    folder_path = f\"version{version}\"\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/lrc.pkl', 'wb') as f:\n",
    "        pickle.dump(lrc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3)\n",
    "    precision = precision_score(y_test, y_pred3)\n",
    "    recall = recall_score(y_test, y_pred3)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/dtc.pkl', 'wb') as f:\n",
    "        pickle.dump(dtc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Support Vector Machine\n",
    "    start_train = time.time()\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred4=svc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred4)\n",
    "    f1 = f1_score(y_test,y_pred4)\n",
    "    precision = precision_score(y_test, y_pred4)\n",
    "    recall = recall_score(y_test, y_pred4)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/svc.pkl', 'wb') as f:\n",
    "        pickle.dump(svc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Gradient Boost\n",
    "    start_train = time.time()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred5=gbc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5)\n",
    "    precision = precision_score(y_test, y_pred5)\n",
    "    recall = (recall_score(y_test, y_pred5))\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/malware/models/gbc.pkl', 'wb') as f:\n",
    "        pickle.dump(gbc, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    start_train = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred6=gnb.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred6)\n",
    "    f1 = f1_score(y_test, y_pred6)\n",
    "    precision = precision_score(y_test,y_pred6)\n",
    "    recall = recall_score(y_test, y_pred6)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "    with open('./tmp/malware/models/gnb.pkl', 'wb') as f:\n",
    "        pickle.dump(gnb, f)      \n",
    "    s3_client.upload_file(\"tmp/malware/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Artificial Neural Network\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    start_train = time.time()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.build()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "    end_train=time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred7 = model.predict(X_test)\n",
    "    y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "    end_test = time.time()\n",
    "    print(y_pred7)\n",
    "    accuracy = accuracy_score(y_test,y_pred7)\n",
    "    f1 = f1_score(y_test, y_pred7)\n",
    "    precision = precision_score(y_test,y_pred7)\n",
    "    recall = recall_score(y_test, y_pred7)\n",
    "    # accuracy = history.history['accuracy'][11]\n",
    "    metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "    with open('./tmp/malware/models/ann.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    s3_client.upload_file(\"tmp/malware/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (name, version) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "        # Iterate through DataFrame rows and insert into the table\n",
    "        for index, row in metrics.iterrows():\n",
    "            cursor.execute(insert_query, (\n",
    "                row['Model'], \n",
    "                row['Version'], \n",
    "                f\"s3://mlpipelineews/version{version}/{row['Model']}/model.pkl\", \n",
    "                False, \n",
    "                row['Accuracy'], \n",
    "                row['F1'], \n",
    "                row['Precision'], \n",
    "                row['Recall'], \n",
    "                row['Train_Time'], \n",
    "                row['Test_Time']\n",
    "            ))\n",
    "    \n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171982d3-0c3b-4e32-8b06-890cdd533c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 20:04:21.342946: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 20:04:21.562764: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2024-07-10 20:04:21.562919: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2024-07-10 20:04:21.589583: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './tmp/malware/models' already exists.\n",
      "Connected to PostgreSQL successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "2024-07-10 20:08:19.960961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 20:08:19.961194: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "432/432 [==============================] - 4s 7ms/step - loss: 0.0801 - accuracy: 0.9762 - val_loss: 0.0434 - val_accuracy: 0.9855\n",
      "Epoch 2/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0439 - accuracy: 0.9862 - val_loss: 0.0397 - val_accuracy: 0.9871\n",
      "Epoch 3/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0383 - accuracy: 0.9876 - val_loss: 0.0400 - val_accuracy: 0.9871\n",
      "Epoch 4/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0353 - accuracy: 0.9883 - val_loss: 0.0346 - val_accuracy: 0.9891\n",
      "Epoch 5/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0332 - accuracy: 0.9888 - val_loss: 0.0339 - val_accuracy: 0.9895\n",
      "Epoch 6/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0319 - accuracy: 0.9893 - val_loss: 0.0332 - val_accuracy: 0.9898\n",
      "Epoch 7/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0323 - val_accuracy: 0.9894\n",
      "Epoch 8/25\n",
      "432/432 [==============================] - 2s 6ms/step - loss: 0.0294 - accuracy: 0.9899 - val_loss: 0.0322 - val_accuracy: 0.9897\n",
      "Epoch 9/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.0309 - val_accuracy: 0.9900\n",
      "Epoch 10/25\n",
      "432/432 [==============================] - 2s 6ms/step - loss: 0.0271 - accuracy: 0.9907 - val_loss: 0.0304 - val_accuracy: 0.9907\n",
      "Epoch 11/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.0293 - val_accuracy: 0.9911\n",
      "Epoch 12/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.0280 - val_accuracy: 0.9914\n",
      "Epoch 13/25\n",
      "432/432 [==============================] - 2s 6ms/step - loss: 0.0253 - accuracy: 0.9914 - val_loss: 0.0286 - val_accuracy: 0.9911\n",
      "Epoch 14/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0246 - accuracy: 0.9914 - val_loss: 0.0275 - val_accuracy: 0.9912\n",
      "Epoch 15/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0239 - accuracy: 0.9920 - val_loss: 0.0277 - val_accuracy: 0.9914\n",
      "Epoch 16/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0234 - accuracy: 0.9919 - val_loss: 0.0282 - val_accuracy: 0.9914\n",
      "Epoch 17/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0230 - accuracy: 0.9921 - val_loss: 0.0280 - val_accuracy: 0.9915\n",
      "Epoch 18/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.0270 - val_accuracy: 0.9914\n",
      "Epoch 19/25\n",
      "432/432 [==============================] - 2s 6ms/step - loss: 0.0216 - accuracy: 0.9926 - val_loss: 0.0282 - val_accuracy: 0.9916\n",
      "Epoch 20/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.0272 - val_accuracy: 0.9917\n",
      "Epoch 21/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0262 - val_accuracy: 0.9922\n",
      "Epoch 22/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0205 - accuracy: 0.9927 - val_loss: 0.0293 - val_accuracy: 0.9911\n",
      "Epoch 23/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 0.0258 - val_accuracy: 0.9919\n",
      "Epoch 24/25\n",
      "432/432 [==============================] - 3s 6ms/step - loss: 0.0199 - accuracy: 0.9929 - val_loss: 0.0274 - val_accuracy: 0.9915\n",
      "Epoch 25/25\n",
      "432/432 [==============================] - 2s 6ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.0262 - val_accuracy: 0.9916\n",
      "863/863 [==============================] - 2s 2ms/step\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 240710 20:09:29 builder_impl:779] Assets written to: ram://8b27c8cf-3e49-45f7-8120-0e5b5cceae7b/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully.\n",
      "Data inserted successfully.\n",
      "PostgreSQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "train_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39d0e3af-4f3b-4f15-a764-8686a6b83776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_deploy() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    import json\n",
    "    import os \n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import boto3\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM model_metrics ORDER BY created_at DESC LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    #print(f\"df = {df['version'][0]}\")\n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "        \n",
    "    #print(f\"version = {version}\")\n",
    "    \n",
    "    try:\n",
    "        fetch_query = f\"SELECT * FROM model_metrics where version={version} order by accuracy desc limit 1;\"\n",
    "        model = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    \n",
    "    name = f\"{model['name'][0]}-version{version}\"\n",
    "    print(name)\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "    \n",
    "    isvc2 = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=name,\n",
    "            namespace=namespace,\n",
    "            annotations={'sidecar.istio.io/inject': 'false'}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=\"s3-service-account\",\n",
    "                sklearn=V1beta1SKLearnSpec(\n",
    "                    storage_uri=model['uri'][0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc2)\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM model_metrics where in_use is true LIMIT 1;\"\n",
    "        old_model = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    update_query_new = \"\"\"\n",
    "        UPDATE model_metrics\n",
    "        SET in_use = true\n",
    "        WHERE name = %s and version = %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    update_query_old = \"\"\"\n",
    "        UPDATE model_metrics\n",
    "        SET version = false\n",
    "        WHERE name = %s and version = %s;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(update_query_new, (model['name'][0], int(model['version'][0])))\n",
    "        if(not old_model.empty):\n",
    "            cursor.execute(update_query_old, (old_model['name'][0], int(old_model['version'][0])))\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    if(not old_model.empty):\n",
    "        del_name = f\"{old_model['name']}-version{old_model['version']}\"\n",
    "        namespace = utils.get_default_target_namespace()\n",
    "\n",
    "        # Initialize the KServe client\n",
    "        KServe = KServeClient()\n",
    "\n",
    "        # Delete the inference service\n",
    "        KServe.delete(del_name, namespace)\n",
    "    \n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd3ce36e-05e5-4aca-8798-6e4ceb68fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully.\n",
      "rfc-version1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_eval_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57547dc6-5c21-4aca-8fbc-8b8c40343b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "read_csv_op = components.func_to_container_op(func=read_file, output_component_file='preprocess.yaml', base_image='python:3.7', packages_to_install=['pandas','scikit-learn==1.0.1', 'kfp', 'numpy', 'minio'])\n",
    "\n",
    "train_op = components.func_to_container_op(func=train_op, output_component_file='train.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow'])\n",
    "\n",
    "read_data_op = kfp.components.load_component_from_file('preprocess.yaml')\n",
    "train_op = kfp.components.load_component_from_file('train.yaml')\n",
    "\n",
    "# @dsl.pipeline(\n",
    "#     name='Machine Learning Pipeline',\n",
    "#     description='A pipeline to preprocess, train, and predict using sklearn and tensorflow'\n",
    "# )\n",
    "\n",
    "def ml_pipeline():\n",
    "    preprocess = read_csv_op()\n",
    "    train = train_op().after(preprocess)\n",
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(ml_pipeline, 'malware_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e85c7-1c7c-4e1a-816a-273d8743d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_query_new = \"\"\"\n",
    "    UPDATE model_metrics\n",
    "    SET in_use = true\n",
    "    WHERE name = '%s' and version = %s;\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
