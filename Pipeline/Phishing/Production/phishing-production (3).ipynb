{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60d08060-29bd-4983-96da-a0834d246bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.0.1 in /usr/local/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (3.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn==1.0.1) (1.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.9/site-packages (2.9.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-2.0.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/site-packages (from sqlalchemy) (3.0.3)\n",
      "Installing collected packages: sqlalchemy\n",
      "Successfully installed sqlalchemy-2.0.31\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1\n",
    "!pip install pandas \n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38aee2ad-f339-4adf-a2e9-062de0a26bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition() -> bool:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import boto3\n",
    "    import json\n",
    "\n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "\n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}', connect_args={'connect_timeout': 60})\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('select count(*) from phishing_data as phd join phishing_outcomes as pho on pho.uid = phd.uid where phd.outcome!=2;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT count(*) FROM metadata_table_phishing;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            meta_count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('select count(*) from phishing_data as phd join phishing_outcomes as pho on pho.uid = phd.uid where pho.outcome!=phd.outcome and phd.outcome!=2;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            amount_incorrect = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "        \n",
    "    if count >= 10 or amount_incorrect > 2:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                delete_query = text(\"DELETE FROM phishing_outcomes pho USING phishing_data phd WHERE pho.uid = phd.uid;\")\n",
    "                result = conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            print(\"error\")\n",
    "        \n",
    "    if (count >= 10 and count != 0) or meta_count == 0 or amount_incorrect > 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67efc116-f59c-47bf-ada1-17d8ec9b6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    preprocess_df = {'version':1}\n",
    "    \n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "    def preprocess(df):\n",
    "        df = df.drop(columns=['url'])\n",
    "        preprocess_df['url'] = None\n",
    "        \n",
    "        for c in df.columns:\n",
    "            if len(df[c].unique()) == 1:\n",
    "                preprocess_df[c] = None\n",
    "                df.drop(columns=[c], inplace=True)\n",
    "        \n",
    "        corr_matrix = df.corr()\n",
    "        target_corr = corr_matrix['outcome']\n",
    "        threshold=0.1\n",
    "        drop_features = target_corr[abs(target_corr)<=threshold].index.tolist()\n",
    "        for i in drop_features:\n",
    "            preprocess_df[i] = None\n",
    "        df.drop(columns=drop_features, inplace=True)\n",
    "        \n",
    "        for i in df.columns:\n",
    "            if i != 'outcome':\n",
    "                zscore_normalization(df, i)\n",
    "                \n",
    "        return df\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "            \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM phishing_data WHERE outcome != 2;')\n",
    "            chunksize = 10000 \n",
    "\n",
    "            chunks = pd.read_sql_query(query, conn, chunksize=chunksize)\n",
    "\n",
    "            features_list = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                features_df = pd.json_normalize(chunk['features'])\n",
    "                features_df['outcome'] = chunk['outcome']\n",
    "                \n",
    "                df = pd.concat([df, features_df], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "\n",
    "    df = preprocess(df)\n",
    "    \n",
    "    X = df.drop(columns=['outcome'])\n",
    "    y = df['outcome']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    bucket_name=\"phishingpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    print(s3_client)\n",
    "    \n",
    "    folder_path = './tmp/phishing'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM metadata_table_phishing ORDER BY version DESC LIMIT 1;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            version = data['version'].iloc[0] + 1\n",
    "            print(version)\n",
    "    except Exception as e:\n",
    "        version = 1\n",
    "    \n",
    "    df.to_csv(\"./tmp/phishing/phishing_data.csv\")\n",
    "    s3_client.upload_file(\"./tmp/phishing/phishing_data.csv\", bucket_name, f\"version{version}/phishing_dataset.csv\")\n",
    "    np.save(\"./tmp/phishing/X_train.npy\",X_train)\n",
    "    s3_client.upload_file(\"./tmp/phishing/X_train.npy\", bucket_name, f\"version{version}/X_train.npy\")\n",
    "    np.save(\"./tmp/phishing/y_train.npy\",y_train)\n",
    "    s3_client.upload_file(\"./tmp/phishing/y_train.npy\", bucket_name, f\"version{version}/y_train.npy\")\n",
    "    np.save(\"./tmp/phishing/X_test.npy\",X_test)\n",
    "    s3_client.upload_file(\"./tmp/phishing/X_test.npy\", bucket_name, f\"version{version}/X_test.npy\")\n",
    "    np.save(\"./tmp/phishing/y_test.npy\",y_test)\n",
    "    s3_client.upload_file(\"./tmp/phishing/y_test.npy\", bucket_name, f\"version{version}/y_test.npy\")\n",
    "        \n",
    "\n",
    "    preprocess_df['version'] = version\n",
    "    mean_df = pd.DataFrame([preprocess_df])\n",
    "    meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), len(X.columns), json.dumps(df.dtypes.astype(str).to_dict()),mean_df.iloc[0].to_json()]], columns = ['version', 'date', 'features', 'types','factor'])\n",
    "    meta_df.to_sql(\"metadata_table_phishing\", engine, if_exists='append', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "764ab714-4185-4e07-8242-46178dd161af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"phishingpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM metadata_table_phishing ORDER BY date DESC LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "        \n",
    "    folder_path = f\"version{version}\"\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"version{version}/X_train.npy\")\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_train = np.load(BytesIO(data))\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_test = np.load(BytesIO(data))\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/phishing/models'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/lrc.pkl', 'wb') as f:\n",
    "        pickle.dump(lrc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3)\n",
    "    precision = precision_score(y_test, y_pred3)\n",
    "    recall = recall_score(y_test, y_pred3)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/dtc.pkl', 'wb') as f:\n",
    "        pickle.dump(dtc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Support Vector Machine\n",
    "    start_train = time.time()\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred4=svc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred4)\n",
    "    f1 = f1_score(y_test,y_pred4)\n",
    "    precision = precision_score(y_test, y_pred4)\n",
    "    recall = recall_score(y_test, y_pred4)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/svc.pkl', 'wb') as f:\n",
    "        pickle.dump(svc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Gradient Boost\n",
    "    start_train = time.time()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred5=gbc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5)\n",
    "    precision = precision_score(y_test, y_pred5)\n",
    "    recall = (recall_score(y_test, y_pred5))\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/gbc.pkl', 'wb') as f:\n",
    "        pickle.dump(gbc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    start_train = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred6=gnb.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred6)\n",
    "    f1 = f1_score(y_test, y_pred6)\n",
    "    precision = precision_score(y_test,y_pred6)\n",
    "    recall = recall_score(y_test, y_pred6)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "    with open('./tmp/phishing/models/gnb.pkl', 'wb') as f:\n",
    "        pickle.dump(gnb, f)      \n",
    "    s3_client.upload_file(\"tmp/phishing/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Artificial Neural Network\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    start_train = time.time()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.build()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "    end_train=time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred7 = model.predict(X_test)\n",
    "    y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "    end_test = time.time()\n",
    "    print(y_pred7)\n",
    "    accuracy = accuracy_score(y_test,y_pred7)\n",
    "    f1 = f1_score(y_test, y_pred7)\n",
    "    precision = precision_score(y_test,y_pred7)\n",
    "    recall = recall_score(y_test, y_pred7)\n",
    "    # accuracy = history.history['accuracy'][11]\n",
    "    metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "    with open('./tmp/phishing/models/ann.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO phishing_model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (name, version) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "        # Iterate through DataFrame rows and insert into the table\n",
    "        for index, row in metrics.iterrows():\n",
    "            cursor.execute(insert_query, (\n",
    "                row['Model'], \n",
    "                row['Version'], \n",
    "                f\"s3://phishingpipeline/version{version}/{row['Model']}/model.pkl\", \n",
    "                False, \n",
    "                row['Accuracy'], \n",
    "                row['F1'], \n",
    "                row['Precision'], \n",
    "                row['Recall'], \n",
    "                row['Train_Time'], \n",
    "                row['Test_Time']\n",
    "            ))\n",
    "    \n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3c87417-7470-48da-a46d-df5fe7ea27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_deploy() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    import json\n",
    "    import os \n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import boto3\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM phishing_model_metrics ORDER BY created_at desc LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\") \n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "    \n",
    "    try:\n",
    "        fetch_query = f\"SELECT * FROM phishing_model_metrics where version={version} order by accuracy desc limit 1;\"\n",
    "        model = pd.read_sql(fetch_query, conn)\n",
    "        accuracy = model['accuracy'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM phishing_model_metrics where in_use is true LIMIT 1;\"\n",
    "        old_model = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\") \n",
    "    \n",
    "    \n",
    "    if accuracy >= .85:\n",
    "        # Query to fetch data from the table\n",
    "\n",
    "        name = f\"{model['name'][0]}-version{version}-phd\"\n",
    "        print(name)\n",
    "        namespace = utils.get_default_target_namespace()\n",
    "        kserve_version='v1beta1'\n",
    "        api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "        isvc2 = V1beta1InferenceService(\n",
    "            api_version=api_version,\n",
    "            kind=constants.KSERVE_KIND,\n",
    "            metadata=client.V1ObjectMeta(\n",
    "                name=name,\n",
    "                namespace=namespace,\n",
    "                annotations={'sidecar.istio.io/inject': 'false'}\n",
    "            ),\n",
    "            spec=V1beta1InferenceServiceSpec(\n",
    "                predictor=V1beta1PredictorSpec(\n",
    "                    service_account_name=\"s3-service-account\",\n",
    "                    sklearn=V1beta1SKLearnSpec(\n",
    "                        storage_uri=model['uri'][0]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        KServe = KServeClient()\n",
    "        KServe.create(isvc2)\n",
    "\n",
    "\n",
    "\n",
    "        update_query_new = \"\"\"\n",
    "            UPDATE phishing_model_metrics\n",
    "            SET in_use = true\n",
    "            WHERE name = %s and version = %s;\n",
    "        \"\"\"\n",
    "\n",
    "        update_query_old = \"\"\"\n",
    "            UPDATE phishing_model_metrics\n",
    "            SET in_use = false\n",
    "            WHERE name = %s and version = %s;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor.execute(update_query_new, (model['name'][0], int(model['version'][0])))\n",
    "            if(not old_model.empty):\n",
    "                cursor.execute(update_query_old, (old_model['name'][0], int(old_model['version'][0])))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "        if(not old_model.empty):\n",
    "            del_name = f\"{old_model['name'][0]}-version{old_model['version'][0]}-phd\"\n",
    "            namespace = utils.get_default_target_namespace()\n",
    "\n",
    "            # Initialize the KServe client\n",
    "            KServe = KServeClient()\n",
    "\n",
    "            # Delete the inference service\n",
    "            KServe.delete(del_name, namespace)\n",
    "    else:\n",
    "        print(\"Bad Accuracy: Email Dovelopers!\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "307d9efa-7d09-47db-a76e-d98456defd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "check_condition_op = components.func_to_container_op(func=check_condition, base_image='python:3.7', packages_to_install=['pandas==1.1.5', 'sqlalchemy==1.4.45', 'boto3', 'psycopg2-binary'])\n",
    "\n",
    "read_csv_op = components.func_to_container_op(func=read_file, output_component_file='preprocess.yaml', base_image='python:3.7', packages_to_install=['pandas==1.1.5','scikit-learn==1.0.1', 'kfp', 'numpy', 'minio', 'psycopg2-binary', 'sqlalchemy==1.4.45','boto3'])\n",
    "\n",
    "train_op = components.func_to_container_op(func=train_op, output_component_file='train.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow', 'psycopg2-binary', 'sqlalchemy','boto3'])\n",
    "\n",
    "eval_deploy = components.func_to_container_op(func=model_eval_deploy, output_component_file='eval_deploy.yaml', base_image='python:3.7', packages_to_install=['pandas', 'scikit-learn==1.0.1','numpy','minio', 'tensorflow', 'psycopg2-binary', 'sqlalchemy','boto3','kubernetes','kserve'])\n",
    "\n",
    "read_data_op = kfp.components.load_component_from_file('preprocess.yaml')\n",
    "train_op = kfp.components.load_component_from_file('train.yaml')\n",
    "eval_deploy_op = kfp.components.load_component_from_file('eval_deploy.yaml')\n",
    "# @dsl.pipeline(\n",
    "#     name='Machine Learning Pipeline',\n",
    "#     description='A pipeline to preprocess, train, and predict using sklearn and tensorflow'\n",
    "# )\n",
    "\n",
    "def ml_pipeline():\n",
    "    check_condition = check_condition_op()\n",
    "    check_condition.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    with dsl.Condition(check_condition.output == 'True'):\n",
    "        preprocess = read_csv_op()\n",
    "        preprocess.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        train = train_op().after(preprocess)\n",
    "        train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        eval_deploy = eval_deploy_op().after(train)\n",
    "        eval_deploy.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(ml_pipeline, 'phishing_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
