{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21dffb2e-e13d-4182-b312-62fb74e8aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(is_experiment: bool = False) -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    preprocess_df = {'version':1}\n",
    "    \n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "    def preprocess(df):\n",
    "        df = df.drop(columns=['url'])\n",
    "        preprocess_df['url'] = None\n",
    "        \n",
    "        for c in df.columns:\n",
    "            if len(df[c].unique()) == 1:\n",
    "                preprocess_df[c] = None\n",
    "                df.drop(columns=[c], inplace=True)\n",
    "        \n",
    "        corr_matrix = df.corr()\n",
    "        target_corr = corr_matrix['outcome']\n",
    "        threshold=0.1\n",
    "        drop_features = target_corr[abs(target_corr)<=threshold].index.tolist()\n",
    "        for i in drop_features:\n",
    "            preprocess_df[i] = None\n",
    "        df.drop(columns=drop_features, inplace=True)\n",
    "        \n",
    "        for i in df.columns:\n",
    "            if i != 'outcome':\n",
    "                zscore_normalization(df, i)\n",
    "                \n",
    "        return df\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "            \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM phishing_data WHERE outcome != 2;')\n",
    "            chunksize = 10000 \n",
    "\n",
    "            chunks = pd.read_sql_query(query, conn, chunksize=chunksize)\n",
    "\n",
    "            features_list = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                features_df = pd.json_normalize(chunk['features'])\n",
    "                features_df['outcome'] = chunk['outcome']\n",
    "                \n",
    "                df = pd.concat([df, features_df], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "\n",
    "    df = preprocess(df)\n",
    "    \n",
    "    X = df.drop(columns=['outcome'])\n",
    "    y = df['outcome']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    bucket_name=\"phishingpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    print(s3_client)\n",
    "    \n",
    "    folder_path = './tmp/phishing'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    df.to_csv(\"./tmp/phishing/phishing_data.csv\")\n",
    "    np.save(\"./tmp/phishing/X_train.npy\",X_train)\n",
    "    np.save(\"./tmp/phishing/y_train.npy\",y_train)\n",
    "    np.save(\"./tmp/phishing/X_test.npy\",X_test)  \n",
    "    np.save(\"./tmp/phishing/y_test.npy\",y_test)\n",
    "    \n",
    "        \n",
    "    if(not is_experiment):\n",
    "        \n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                query = text('SELECT * FROM metadata_table_phishing ORDER BY version DESC LIMIT 1;')\n",
    "                data = pd.read_sql_query(query, conn)\n",
    "                version = data['version'].iloc[0] + 1\n",
    "                print(version)\n",
    "        except Exception as e:\n",
    "            version = 1\n",
    "        \n",
    "        s3_client.upload_file(\"./tmp/phishing/phishing_data.csv\", bucket_name, f\"version{version}/phishing_dataset.csv\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/X_train.npy\", bucket_name, f\"version{version}/X_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/y_train.npy\", bucket_name, f\"version{version}/y_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/X_test.npy\", bucket_name, f\"version{version}/X_test.npy\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/y_test.npy\", bucket_name, f\"version{version}/y_test.npy\")\n",
    "        \n",
    "        preprocess_df['version'] = version\n",
    "        mean_df = pd.DataFrame([preprocess_df])\n",
    "        meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), len(X.columns), json.dumps(df.dtypes.astype(str).to_dict()),mean_df.iloc[0].to_json()]], columns = ['version', 'date', 'features', 'types','factor'])\n",
    "        meta_df.to_sql(\"metadata_table_phishing\", engine, if_exists='append', index=False)\n",
    "    else:\n",
    "        s3_client.upload_file(\"./tmp/phishing/phishing_data.csv\", bucket_name, f\"experiment/phishing_dataset.csv\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/X_train.npy\", bucket_name, f\"experiment/X_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/y_train.npy\", bucket_name, f\"experiment/y_train.npy\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/X_test.npy\", bucket_name, f\"experiment/X_test.npy\")\n",
    "        s3_client.upload_file(\"./tmp/phishing/y_test.npy\", bucket_name, f\"experiment/y_test.npy\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd08ad1-3439-4349-be72-97adc552d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(is_experiment: bool = False) -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"phishingpipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    if(not is_experiment):\n",
    "        db_details = {\n",
    "            'dbname': db,\n",
    "            'user': user,\n",
    "            'password': pswd,\n",
    "            'host': host,\n",
    "            'port': port\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # Connect to PostgreSQL\n",
    "        try:\n",
    "            conn = psycopg2.connect(**db_details)\n",
    "            cursor = conn.cursor()\n",
    "            print(\"Connected to PostgreSQL successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "            exit()\n",
    "\n",
    "        # Query to fetch data from the table\n",
    "        try:\n",
    "            fetch_query = \"SELECT * FROM metadata_table_phishing ORDER BY date DESC LIMIT 1;\"\n",
    "            df = pd.read_sql(fetch_query, conn)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "        if(not df.empty):\n",
    "            version = df['version'][0]\n",
    "        else:\n",
    "            version = 1\n",
    "\n",
    "        folder_path = f\"version{version}\"\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"version{version}/X_train.npy\")\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_train = np.load(BytesIO(data))\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_train = np.load(BytesIO(data))\n",
    "\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_test = np.load(BytesIO(data))\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    else:\n",
    "        version = 0\n",
    "        folder_path = 'experiment'\n",
    "    \n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/X_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_train = np.load(BytesIO(data))\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/y_train.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_train = np.load(BytesIO(data))\n",
    "\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/X_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        X_test = np.load(BytesIO(data))\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=f\"experiment/y_test.npy\")\n",
    "        data = response['Body'].read()\n",
    "        y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/phishing/models'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    \n",
    "    #Logistic Regression\n",
    "    start_train = time.time()\n",
    "    lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    lrc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    ypredlr = lrc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, ypredlr)\n",
    "    f1 = f1_score(y_test, ypredlr)\n",
    "    precision = precision_score(y_test, ypredlr)\n",
    "    recall = recall_score(y_test, ypredlr)\n",
    "    metrics.loc[len(metrics.index)] = [version,'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/lrc.pkl', 'wb') as f:\n",
    "        pickle.dump(lrc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    f1 = f1_score(y_test, y_pred2)\n",
    "    precision = precision_score(y_test, y_pred2)\n",
    "    recall = recall_score(y_test, y_pred2)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3)\n",
    "    precision = precision_score(y_test, y_pred3)\n",
    "    recall = recall_score(y_test, y_pred3)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/dtc.pkl', 'wb') as f:\n",
    "        pickle.dump(dtc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Support Vector Machine\n",
    "    start_train = time.time()\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred4=svc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred4)\n",
    "    f1 = f1_score(y_test,y_pred4)\n",
    "    precision = precision_score(y_test, y_pred4)\n",
    "    recall = recall_score(y_test, y_pred4)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'svc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/svc.pkl', 'wb') as f:\n",
    "        pickle.dump(svc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/svc.pkl\", bucket_name, f\"{folder_path}/svc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Gradient Boost\n",
    "    start_train = time.time()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred5=gbc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5)\n",
    "    precision = precision_score(y_test, y_pred5)\n",
    "    recall = (recall_score(y_test, y_pred5))\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gbc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/phishing/models/gbc.pkl', 'wb') as f:\n",
    "        pickle.dump(gbc, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/gbc.pkl\", bucket_name, f\"{folder_path}/gbc/model.pkl\")\n",
    "    \n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    start_train = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred6=gnb.predict(X_test)\n",
    "    end_test = time.time()\n",
    "    accuracy = accuracy_score(y_test,y_pred6)\n",
    "    f1 = f1_score(y_test, y_pred6)\n",
    "    precision = precision_score(y_test,y_pred6)\n",
    "    recall = recall_score(y_test, y_pred6)\n",
    "    metrics.loc[len(metrics.index)] = [version, 'gnb', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]  \n",
    "    with open('./tmp/phishing/models/gnb.pkl', 'wb') as f:\n",
    "        pickle.dump(gnb, f)      \n",
    "    s3_client.upload_file(\"tmp/phishing/models/gnb.pkl\", bucket_name, f\"{folder_path}/gnb/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Artificial Neural Network\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    start_train = time.time()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.build()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=256, epochs=25)\n",
    "    end_train=time.time()\n",
    "    start_test = time.time()\n",
    "    y_pred7 = model.predict(X_test)\n",
    "    y_pred7 = (y_pred7 > 0.5).astype(np.int32)\n",
    "    end_test = time.time()\n",
    "    print(y_pred7)\n",
    "    accuracy = accuracy_score(y_test,y_pred7)\n",
    "    f1 = f1_score(y_test, y_pred7)\n",
    "    precision = precision_score(y_test,y_pred7)\n",
    "    recall = recall_score(y_test, y_pred7)\n",
    "    # accuracy = history.history['accuracy'][11]\n",
    "    metrics.loc[len(metrics.index)] = [version, 'ann', accuracy, f1, precision, recall, end_test-start_test, 0]\n",
    "    with open('./tmp/phishing/models/ann.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    s3_client.upload_file(\"tmp/phishing/models/ann.pkl\", bucket_name, f\"{folder_path}/ann/model.pkl\")\n",
    "\n",
    "    if(not is_experiment):\n",
    "        db_details = {\n",
    "            'dbname': db,\n",
    "            'user': user,\n",
    "            'password': pswd,\n",
    "            'host': host,\n",
    "            'port': port\n",
    "        }\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO phishing_model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (name, version) DO NOTHING;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = psycopg2.connect(**db_details)\n",
    "            cursor = conn.cursor()\n",
    "            print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "            # Iterate through DataFrame rows and insert into the table\n",
    "            for index, row in metrics.iterrows():\n",
    "                cursor.execute(insert_query, (\n",
    "                    row['Model'], \n",
    "                    row['Version'], \n",
    "                    f\"s3://phishingpipeline/version{version}/{row['Model']}/model.pkl\", \n",
    "                    False, \n",
    "                    row['Accuracy'], \n",
    "                    row['F1'], \n",
    "                    row['Precision'], \n",
    "                    row['Recall'], \n",
    "                    row['Train_Time'], \n",
    "                    row['Test_Time']\n",
    "                ))\n",
    "\n",
    "            conn.commit()\n",
    "            print(\"Data inserted successfully.\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection closed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to PostgreSQL or insert data: {e}\")\n",
    "    else:\n",
    "        print(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7809f98-7440-4a82-9988-6c6136112f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_functions() -> None:\n",
    "    read_file(True)\n",
    "    train_op(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f9a7ee-d82d-4255-9b01-a57e5f119793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.S3 object at 0x7fd627550160>\n",
      "Folder './tmp/phishing' already exists.\n",
      "Folder './tmp/phishing/models' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 19:22:51.266928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 19:22:51.268023: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "36/36 [==============================] - 2s 45ms/step - loss: 0.4515 - accuracy: 0.8138 - val_loss: 0.2927 - val_accuracy: 0.8893\n",
      "Epoch 2/25\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 0.2266 - accuracy: 0.9134 - val_loss: 0.1816 - val_accuracy: 0.9313\n",
      "Epoch 3/25\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.1593 - accuracy: 0.9378 - val_loss: 0.1526 - val_accuracy: 0.9427\n",
      "Epoch 4/25\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.1382 - accuracy: 0.9489 - val_loss: 0.1444 - val_accuracy: 0.9488\n",
      "Epoch 5/25\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 0.1279 - accuracy: 0.9521 - val_loss: 0.1385 - val_accuracy: 0.9506\n",
      "Epoch 6/25\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.1214 - accuracy: 0.9548 - val_loss: 0.1356 - val_accuracy: 0.9510\n",
      "Epoch 7/25\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 0.1149 - accuracy: 0.9571 - val_loss: 0.1338 - val_accuracy: 0.9536\n",
      "Epoch 8/25\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.1097 - accuracy: 0.9605 - val_loss: 0.1322 - val_accuracy: 0.9541\n",
      "Epoch 9/25\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.1056 - accuracy: 0.9617 - val_loss: 0.1312 - val_accuracy: 0.9545\n",
      "Epoch 10/25\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 0.1019 - accuracy: 0.9629 - val_loss: 0.1294 - val_accuracy: 0.9532\n",
      "Epoch 11/25\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 0.0975 - accuracy: 0.9652 - val_loss: 0.1289 - val_accuracy: 0.9536\n",
      "Epoch 12/25\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0944 - accuracy: 0.9662 - val_loss: 0.1301 - val_accuracy: 0.9532\n",
      "Epoch 13/25\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.0909 - accuracy: 0.9680 - val_loss: 0.1297 - val_accuracy: 0.9563\n",
      "Epoch 14/25\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.0878 - accuracy: 0.9683 - val_loss: 0.1261 - val_accuracy: 0.9514\n",
      "Epoch 15/25\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0842 - accuracy: 0.9701 - val_loss: 0.1276 - val_accuracy: 0.9549\n",
      "Epoch 16/25\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 0.0824 - accuracy: 0.9706 - val_loss: 0.1267 - val_accuracy: 0.9545\n",
      "Epoch 17/25\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.0792 - accuracy: 0.9718 - val_loss: 0.1267 - val_accuracy: 0.9536\n",
      "Epoch 18/25\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.0766 - accuracy: 0.9723 - val_loss: 0.1272 - val_accuracy: 0.9536\n",
      "Epoch 19/25\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0753 - accuracy: 0.9727 - val_loss: 0.1262 - val_accuracy: 0.9528\n",
      "Epoch 20/25\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0724 - accuracy: 0.9744 - val_loss: 0.1271 - val_accuracy: 0.9549\n",
      "Epoch 21/25\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0692 - accuracy: 0.9760 - val_loss: 0.1279 - val_accuracy: 0.9532\n",
      "Epoch 22/25\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0668 - accuracy: 0.9760 - val_loss: 0.1282 - val_accuracy: 0.9558\n",
      "Epoch 23/25\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0639 - accuracy: 0.9775 - val_loss: 0.1307 - val_accuracy: 0.9541\n",
      "Epoch 24/25\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - accuracy: 0.9797 - val_loss: 0.1326 - val_accuracy: 0.9528\n",
      "Epoch 25/25\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0596 - accuracy: 0.9808 - val_loss: 0.1318 - val_accuracy: 0.9549\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "INFO:tensorflow:Assets written to: ram://0598068a-3383-4e9e-8b70-5ffd53a9ae1d/assets\n",
      "   Version Model  Accuracy        F1  Precision    Recall  Train_Time  \\\n",
      "0        0   lrc  0.938758  0.937332   0.944094  0.930667    1.441647   \n",
      "1        0   rfc  0.964129  0.963588   0.962733  0.964444    2.105220   \n",
      "2        0   dtc  0.939633  0.938061   0.947416  0.928889    0.216407   \n",
      "3        0   svc  0.955381  0.954383   0.960396  0.948444    1.711653   \n",
      "4        0   gbc  0.951006  0.950045   0.953447  0.946667    5.155077   \n",
      "5        0   gnb  0.753281  0.684916   0.921805  0.544889    0.007924   \n",
      "6        0   ann  0.954943  0.953708   0.964545  0.943111    0.890404   \n",
      "\n",
      "   Test_Time  \n",
      "0   0.002939  \n",
      "1   0.090724  \n",
      "2   0.003174  \n",
      "3   0.466599  \n",
      "4   0.006568  \n",
      "5   0.029220  \n",
      "6   0.000000  \n"
     ]
    }
   ],
   "source": [
    "run_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ae8e2-1ef7-46fa-8301-81c34cedf63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
