{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition() -> bool:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import boto3\n",
    "    import json\n",
    "\n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "\n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}', connect_args={'connect_timeout': 60})\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('select count(*) from cyber_data as cyd join cyber_outcomes as cyo on cyo.uid = cyd.uid where cyd.outcome is not NULL;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT count(*) FROM metadata_table_cyber;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            meta_count = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('select count(*) from cyber_data as cyd join cyber_outcomes as cyo on cyo.uid = cyd.uid where cyo.outcome!=cyd.outcome and cyd.outcome is not NULL;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            amount_incorrect = data.iloc[0]['count']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    if count >= 10 or amount_incorrect > 2:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                delete_query = text(\"DELETE FROM cyber_outcomes cyo USING cyber_data cyd WHERE cyo.uid = cyd.uid;\")\n",
    "                result = conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "    if (count >= 10 and count != 0) or meta_count == 0 or amount_incorrect > 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file() -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    from scipy.special import boxcox\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import boto3\n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    import base64\n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import datetime\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    preprocess_df = {'version':1}\n",
    "    \n",
    "    def zscore_normalization(df, name):\n",
    "        mean = df[name].mean()\n",
    "        sd = df[name].std()\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "        preprocess_df[name] = (mean, sd)\n",
    "\n",
    "    def encode_text(df, name):\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "        enc = OrdinalEncoder()\n",
    "        data = enc.fit_transform(df[name].values.reshape(-1,1))\n",
    "        df[name] = data.flatten()\n",
    "        preprocess_df[name] = base64.b64encode(pickle.dumps(enc)).decode('utf-8')\n",
    "\n",
    "        \n",
    "    def preprocess(df):        \n",
    "        for c in df.columns:\n",
    "            if len(df[c].unique()) == 1:\n",
    "                df.drop(columns=[c], inplace=True)\n",
    "                preprocess_df[c] = None\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col != 'outcome':\n",
    "                t = (df[col].dtype)\n",
    "                if t == 'int64' or t == 'float64':\n",
    "                    df[col] = boxcox(df[col], 0.5)\n",
    "                    zscore_normalization(df, col)\n",
    "                else:\n",
    "                    encode_text(df, col)\n",
    "\n",
    "        df.drop(columns=[\"label\"], inplace=True)\n",
    "        preprocess_df['label'] = None\n",
    "\n",
    "        corr_matrix = df.corr()\n",
    "        target_corr = corr_matrix['outcome']\n",
    "        threshold=0.05\n",
    "        drop_features = target_corr[abs(target_corr)<=threshold].index.tolist()\n",
    "        for i in drop_features:\n",
    "            preprocess_df[i] = None\n",
    "        df.drop(columns=drop_features, inplace=True)\n",
    "                \n",
    "        return df\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{db_details[\"user\"]}:{db_details[\"password\"]}@{db_details[\"host\"]}:{db_details[\"port\"]}/{db_details[\"dbname\"]}')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "            \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM cyber_data WHERE outcome is not NULL;')\n",
    "            chunksize = 10000 \n",
    "\n",
    "            chunks = pd.read_sql_query(query, conn, chunksize=chunksize)\n",
    "\n",
    "            features_list = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                features_df = pd.json_normalize(chunk['features'])\n",
    "                features_df['outcome'] = chunk['outcome']\n",
    "                \n",
    "                df = pd.concat([df, features_df], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "\n",
    "    df = preprocess(df)\n",
    "    \n",
    "    X = df.drop(columns=[\"outcome\"])\n",
    "    y = df[\"outcome\"]\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    bucket_name=\"multiclasspipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    print(s3_client)\n",
    "    \n",
    "    folder_path = './tmp/cyber'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "        \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text('SELECT * FROM metadata_table_cyber ORDER BY version DESC LIMIT 1;')\n",
    "            data = pd.read_sql_query(query, conn)\n",
    "            version = data['version'].iloc[0] + 1\n",
    "            print(version)\n",
    "    except Exception as e:\n",
    "        version = 1\n",
    "    \n",
    "    df.to_csv(\"./tmp/cyber/cyber_data.csv\")\n",
    "    s3_client.upload_file(\"./tmp/cyber/cyber_data.csv\", bucket_name, f\"version{version}/cyber_dataset.csv\")\n",
    "    np.save(\"./tmp/cyber/X_train.npy\",X_train)\n",
    "    s3_client.upload_file(\"./tmp/cyber/X_train.npy\", bucket_name, f\"version{version}/X_train.npy\")\n",
    "    np.save(\"./tmp/cyber/y_train.npy\",y_train)\n",
    "    s3_client.upload_file(\"./tmp/cyber/y_train.npy\", bucket_name, f\"version{version}/y_train.npy\")\n",
    "    np.save(\"./tmp/cyber/X_test.npy\",X_test)\n",
    "    s3_client.upload_file(\"./tmp/cyber/X_test.npy\", bucket_name, f\"version{version}/X_test.npy\")\n",
    "    np.save(\"./tmp/cyber/y_test.npy\",y_test)\n",
    "    s3_client.upload_file(\"./tmp/cyber/y_test.npy\", bucket_name, f\"version{version}/y_test.npy\")\n",
    "        \n",
    "\n",
    "    preprocess_df['version'] = version\n",
    "    mean_df = pd.DataFrame([preprocess_df])\n",
    "    meta_df = pd.DataFrame(data = [[version, datetime.datetime.now(), len(X.columns), json.dumps(df.dtypes.astype(str).to_dict()),mean_df.iloc[0].to_json()]], columns = ['version', 'date', 'features', 'types','factor'])\n",
    "    meta_df.to_sql(\"metadata_table_cyber\", engine, if_exists='append', index=False)\n",
    "\n",
    "#make some changes to the file \n",
    "#run pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    import boto3\n",
    "    from minio import Minio\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import create_engine, Table, Column, Float, Integer, String, MetaData, ARRAY\n",
    "    from sqlalchemy import select, desc, insert, text\n",
    "    from io import BytesIO\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    bucket_name=\"multiclasspipeline\"\n",
    "    role_arn = 'arn:aws:iam::533267059960:role/aws-s3-access'\n",
    "    session_name = 'kubeflow-pipeline-session'\n",
    "    sts_client = boto3.client('sts')\n",
    "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    # Configure AWS SDK with temporary credentials\n",
    "    s3_client = boto3.client('s3',\n",
    "                      aws_access_key_id=credentials['AccessKeyId'],\n",
    "                      aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                      aws_session_token=credentials['SessionToken'])\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Query to fetch data from the table\n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM metadata_table_cyber ORDER BY date DESC LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "        \n",
    "    folder_path = f\"version{version}\"\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"version{version}/X_train.npy\")\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_train = np.load(BytesIO(data))\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_train.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_train = np.load(BytesIO(data))\n",
    "\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/X_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    X_test = np.load(BytesIO(data))\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=f\"version{version}/y_test.npy\")\n",
    "    data = response['Body'].read()\n",
    "    y_test = np.load(BytesIO(data))\n",
    "    \n",
    "    \n",
    "    # Define dataframe to store model metrics\n",
    "    metrics = pd.DataFrame(columns=[\"Version\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Train_Time\", \"Test_Time\"])\n",
    "    models_path = './tmp/cyber/models'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(models_path):\n",
    "        os.makedirs(models_path)\n",
    "        print(f\"Folder '{models_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{models_path}' already exists.\")\n",
    "        \n",
    "    #Random Forest Classifier\n",
    "    start_train = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred2=rfc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred2)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred2, average='macro')\n",
    "    recall = recall_score(y_test, y_pred2, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred2, average=\"macro\")\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    \n",
    "    metrics.loc[len(metrics.index)] = [version, 'rfc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/cyber/models/rfc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/cyber/models/rfc.pkl\", bucket_name, f\"{folder_path}/rfc/model.pkl\")\n",
    "\n",
    "\n",
    "    # Decision Tree\n",
    "\n",
    "    start_train = time.time()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred3=dtc.predict(X_test)\n",
    "    end_test = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred3)\n",
    "    f1 = f1_score(y_test, y_pred3, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred3, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred3, average=\"macro\")\n",
    "\n",
    "    metrics.loc[len(metrics.index)] = [version, 'dtc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/cyber/models/dtc.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/cyber/models/dtc.pkl\", bucket_name, f\"{folder_path}/dtc/model.pkl\")\n",
    "\n",
    "\n",
    "    #KNN\n",
    "\n",
    "    start_train = time.time()\n",
    "    knn = KNeighborsClassifier(n_neighbors=2)\n",
    "    knn.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred4=knn.predict(X_test)\n",
    "    end_test = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred4)\n",
    "    f1 = f1_score(y_test, y_pred4, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred4, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred4, average=\"macro\")\n",
    "\n",
    "    metrics.loc[len(metrics.index)] = [version, 'knn', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/cyber/models/knn.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/cyber/models/knn.pkl\", bucket_name, f\"{folder_path}/knn/model.pkl\")\n",
    "\n",
    "    #SGD\n",
    "\n",
    "    start_train = time.time()\n",
    "    sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    sgd.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred5=sgd.predict(X_test)\n",
    "    end_test = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred5)\n",
    "    f1 = f1_score(y_test, y_pred5, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred5, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred5, average=\"macro\")\n",
    "\n",
    "    metrics.loc[len(metrics.index)] = [version, 'sgd', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    with open('./tmp/cyber/models/sgd.pkl', 'wb') as f:\n",
    "        pickle.dump(rfc, f)\n",
    "    s3_client.upload_file(\"tmp/cyber/models/sgd.pkl\", bucket_name, f\"{folder_path}/sgd/model.pkl\")\n",
    "\n",
    "    #Logistic Regression\n",
    "\n",
    "    # start_train = time.time()\n",
    "    # lrc = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    # lrc.fit(X_train, y_train)\n",
    "    # end_train = time.time()\n",
    "\n",
    "    # start_test = time.time()\n",
    "    # y_pred6=lrc.predict(X_test)\n",
    "    # end_test = time.time()\n",
    "\n",
    "    # accuracy = accuracy_score(y_test, y_pred6)\n",
    "    # f1 = f1_score(y_test, y_pred6)\n",
    "    # precision = precision_score(y_test, y_pred6)\n",
    "    # recall = recall_score(y_test, y_pred6)\n",
    "\n",
    "    # metrics.loc[len(metrics.index)] = [version, 'lrc', accuracy, f1, precision, recall, end_train-start_train, end_test-start_test]\n",
    "    # with open('./tmp/cyber/models/lrc.pkl', 'wb') as f:\n",
    "    #     pickle.dump(rfc, f)\n",
    "    # s3_client.upload_file(\"tmp/cyber/models/lrc.pkl\", bucket_name, f\"{folder_path}/lrc/model.pkl\")\n",
    "\n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "        \n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO cyber_model_metrics (name, version, URI, in_use, accuracy, f1, precision, recall, train_time, test_time)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (name, version) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "\n",
    "        # Iterate through DataFrame rows and insert into the table\n",
    "        for index, row in metrics.iterrows():\n",
    "            cursor.execute(insert_query, (\n",
    "                row['Model'], \n",
    "                row['Version'], \n",
    "                f\"s3://multiclasspipeline/version{version}/{row['Model']}/model.pkl\", \n",
    "                False, \n",
    "                row['Accuracy'], \n",
    "                row['F1'], \n",
    "                row['Precision'], \n",
    "                row['Recall'], \n",
    "                row['Train_Time'], \n",
    "                row['Test_Time']\n",
    "            ))\n",
    "    \n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL or insert data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_deploy() -> None:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    import json\n",
    "    import os \n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import boto3\n",
    "    \n",
    "    import psycopg2\n",
    "    from psycopg2 import sql\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    \n",
    "    def get_secret():\n",
    "\n",
    "        secret_name = \"DBCreds\"\n",
    "        region_name = \"us-east-1\"\n",
    "\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            get_secret_value_response = client.get_secret_value(\n",
    "                SecretId=secret_name\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise e\n",
    "\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "        # Parse the secret string to get the credentials\n",
    "        secret_dict = json.loads(secret)\n",
    "        username = secret_dict['username']\n",
    "        password = secret_dict['password']\n",
    "        host = secret_dict['host']\n",
    "        port = secret_dict['port']\n",
    "        dbname = secret_dict['dbname']\n",
    "\n",
    "        return username, password, host, port, dbname\n",
    "\n",
    "\n",
    "    (user,pswd,host,port,db) = get_secret()\n",
    "    \n",
    "    \n",
    "    db_details = {\n",
    "        'dbname': db,\n",
    "        'user': user,\n",
    "        'password': pswd,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_details)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        exit()\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM cyber_model_metrics ORDER BY created_at desc LIMIT 1;\"\n",
    "        df = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\") \n",
    "    \n",
    "    if(not df.empty):\n",
    "        version = df['version'][0]\n",
    "    else:\n",
    "        version = 1\n",
    "    \n",
    "    try:\n",
    "        fetch_query = f\"SELECT * FROM cyber_model_metrics where version={version} order by accuracy desc limit 1;\"\n",
    "        model = pd.read_sql(fetch_query, conn)\n",
    "        accuracy = model['accuracy'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "    \n",
    "    try:\n",
    "        fetch_query = \"SELECT * FROM cyber_model_metrics where in_use is true LIMIT 1;\"\n",
    "        old_model = pd.read_sql(fetch_query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\") \n",
    "    \n",
    "    \n",
    "    if accuracy >= .85:\n",
    "        # Query to fetch data from the table\n",
    "\n",
    "        name = f\"{model['name'][0]}-version{version}-cyd\"\n",
    "        print(name)\n",
    "        namespace = utils.get_default_target_namespace()\n",
    "        kserve_version='v1beta1'\n",
    "        api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "        isvc2 = V1beta1InferenceService(\n",
    "            api_version=api_version,\n",
    "            kind=constants.KSERVE_KIND,\n",
    "            metadata=client.V1ObjectMeta(\n",
    "                name=name,\n",
    "                namespace=namespace,\n",
    "                annotations={'sidecar.istio.io/inject': 'false'}\n",
    "            ),\n",
    "            spec=V1beta1InferenceServiceSpec(\n",
    "                predictor=V1beta1PredictorSpec(\n",
    "                    service_account_name=\"s3-service-account\",\n",
    "                    sklearn=V1beta1SKLearnSpec(\n",
    "                        storage_uri=model['uri'][0]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        KServe = KServeClient()\n",
    "        KServe.create(isvc2)\n",
    "\n",
    "\n",
    "\n",
    "        update_query_new = \"\"\"\n",
    "            UPDATE cyber_model_metrics\n",
    "            SET in_use = true\n",
    "            WHERE name = %s and version = %s;\n",
    "        \"\"\"\n",
    "\n",
    "        update_query_old = \"\"\"\n",
    "            UPDATE cyber_model_metrics\n",
    "            SET in_use = false\n",
    "            WHERE name = %s and version = %s;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor.execute(update_query_new, (model['name'][0], int(model['version'][0])))\n",
    "            if(not old_model.empty):\n",
    "                cursor.execute(update_query_old, (old_model['name'][0], int(old_model['version'][0])))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "        if(not old_model.empty):\n",
    "            del_name = f\"{old_model['name'][0]}-version{old_model['version'][0]}-cyd\"\n",
    "            namespace = utils.get_default_target_namespace()\n",
    "\n",
    "            # Initialize the KServe client\n",
    "            KServe = KServeClient()\n",
    "\n",
    "            # Delete the inference service\n",
    "            KServe.delete(del_name, namespace)\n",
    "    else:\n",
    "        print(\"Bad Accuracy: Email Dovelopers!\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
